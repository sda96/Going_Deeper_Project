{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "capable-moral",
   "metadata": {},
   "source": [
    "[모델 및 코드 출처](https://github.com/Beomi/KcELECTRA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consistent-pharmacy",
   "metadata": {},
   "source": [
    "## 1. 필요한 패키지 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "impossible-entity",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "\n",
    "from pytorch_lightning import LightningModule, Trainer, seed_everything\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AdamW\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "import re\n",
    "import emoji\n",
    "from soynlp.normalizer import repeat_normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distributed-guest",
   "metadata": {},
   "source": [
    "## 2. 사전에 인자들 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "spread-playing",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'random_seed': 42, # Random Seed\n",
    "    'pretrained_model': 'beomi/KcELECTRA-base',  # Transformers PLM name\n",
    "    'pretrained_tokenizer': '',  # Optional, Transformers Tokenizer Name. Overrides `pretrained_model`\n",
    "    'batch_size': 32,\n",
    "    'lr': 5e-6,  # Starting Learning Rate\n",
    "    'epochs': 1,  # Max Epochs\n",
    "    'max_length': 150,  # Max Length input size\n",
    "    'train_data_path': \"nsmc_data/ratings_train.txt\",  # Train Dataset file \n",
    "    'val_data_path': \"nsmc_data/ratings_test.txt\",  # Validation Dataset file \n",
    "    'test_mode': False,  # Test Mode enables `fast_dev_run`\n",
    "    'optimizer': 'AdamW',  # AdamW vs AdamP\n",
    "    'lr_scheduler': 'exp',  # ExponentialLR vs CosineAnnealingWarmRestarts\n",
    "    'fp16': True,  # Enable train on FP16(if GPU)\n",
    "    'tpu_cores': 0,  # Enable TPU with 1 core or 8 cores\n",
    "    'cpu_workers': os.cpu_count(),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "widespread-jungle",
   "metadata": {},
   "source": [
    "## 3. End-to-end 파이프라인 설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "synthetic-magazine",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(LightningModule):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters() # 이 부분에서 self.hparams에 위 kwargs가 저장된다.\n",
    "        \n",
    "        self.clsfier = AutoModelForSequenceClassification.from_pretrained(self.hparams.pretrained_model)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            self.hparams.pretrained_tokenizer\n",
    "            if self.hparams.pretrained_tokenizer\n",
    "            else self.hparams.pretrained_model\n",
    "        )\n",
    "\n",
    "    def forward(self, **kwargs):\n",
    "        return self.clsfier(**kwargs)\n",
    "\n",
    "    def step(self, batch, batch_idx):\n",
    "        data, labels = batch\n",
    "        output = self(input_ids=data, labels=labels)\n",
    "\n",
    "        # Transformers 4.0.0+\n",
    "        loss = output.loss\n",
    "        logits = output.logits\n",
    "\n",
    "        preds = logits.argmax(dim=-1)\n",
    "\n",
    "        y_true = list(labels.cpu().numpy())\n",
    "        y_pred = list(preds.cpu().numpy())\n",
    "\n",
    "        return {\n",
    "            'loss': loss,\n",
    "            'y_true': y_true,\n",
    "            'y_pred': y_pred,\n",
    "        }\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.step(batch, batch_idx)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.step(batch, batch_idx)\n",
    "\n",
    "    def epoch_end(self, outputs, state='train'):\n",
    "        loss = torch.tensor(0, dtype=torch.float)\n",
    "        for i in outputs:\n",
    "            loss += i['loss'].cpu().detach()\n",
    "        loss = loss / len(outputs)\n",
    "\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        for i in outputs:\n",
    "            y_true += i['y_true']\n",
    "            y_pred += i['y_pred']\n",
    "        \n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        prec = precision_score(y_true, y_pred)\n",
    "        rec = recall_score(y_true, y_pred)\n",
    "        f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "        self.log(state+'_loss', float(loss), on_epoch=True, prog_bar=True)\n",
    "        self.log(state+'_acc', acc, on_epoch=True, prog_bar=True)\n",
    "        self.log(state+'_precision', prec, on_epoch=True, prog_bar=True)\n",
    "        self.log(state+'_recall', rec, on_epoch=True, prog_bar=True)\n",
    "        self.log(state+'_f1', f1, on_epoch=True, prog_bar=True)\n",
    "        print(f'[Epoch {self.trainer.current_epoch} {state.upper()}] Loss: {loss}, Acc: {acc}, Prec: {prec}, Rec: {rec}, F1: {f1}')\n",
    "        return {'loss': loss}\n",
    "    \n",
    "    def training_epoch_end(self, outputs):\n",
    "        self.epoch_end(outputs, state='train')\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        self.epoch_end(outputs, state='val')\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        if self.hparams.optimizer == 'AdamW':\n",
    "            optimizer = AdamW(self.parameters(), lr=self.hparams.lr)\n",
    "        elif self.hparams.optimizer == 'AdamP':\n",
    "            from adamp import AdamP\n",
    "            optimizer = AdamP(self.parameters(), lr=self.hparams.lr)\n",
    "        else:\n",
    "            raise NotImplementedError('Only AdamW and AdamP is Supported!')\n",
    "        if self.hparams.lr_scheduler == 'cos':\n",
    "            scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=1, T_mult=2)\n",
    "        elif self.hparams.lr_scheduler == 'exp':\n",
    "            scheduler = ExponentialLR(optimizer, gamma=0.5)\n",
    "        else:\n",
    "            raise NotImplementedError('Only cos and exp lr scheduler is Supported!')\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'scheduler': scheduler,\n",
    "        }\n",
    "\n",
    "    def read_data(self, path):\n",
    "        if path.endswith('xlsx'):\n",
    "            return pd.read_excel(path)\n",
    "        elif path.endswith('csv'):\n",
    "            return pd.read_csv(path)\n",
    "        elif path.endswith('tsv') or path.endswith('txt'):\n",
    "            return pd.read_csv(path, sep='\\t')\n",
    "        else:\n",
    "            raise NotImplementedError('Only Excel(xlsx)/Csv/Tsv(txt) are Supported')\n",
    "\n",
    "    def clean(self, x):\n",
    "        emojis = ''.join(emoji.UNICODE_EMOJI.keys())\n",
    "        pattern = re.compile(f'[^ .,?!/@$%~％·∼()\\x00-\\x7Fㄱ-힣{emojis}]+')\n",
    "        url_pattern = re.compile(\n",
    "            r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)')\n",
    "        x = pattern.sub(' ', x)\n",
    "        x = url_pattern.sub('', x)\n",
    "        x = x.strip()\n",
    "        x = repeat_normalize(x, num_repeats=2)\n",
    "        return x\n",
    "\n",
    "    def encode(self, x, **kwargs):\n",
    "        return self.tokenizer.encode(\n",
    "            self.clean(str(x)),\n",
    "            padding='max_length',\n",
    "            max_length=self.hparams.max_length,\n",
    "            truncation=True,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    def preprocess_dataframe(self, df):\n",
    "        df['document'] = df['document'].map(self.encode)\n",
    "        return df\n",
    "\n",
    "    def dataloader(self, path, shuffle=False):\n",
    "        df = self.read_data(path)\n",
    "        df = self.preprocess_dataframe(df)\n",
    "\n",
    "        dataset = TensorDataset(\n",
    "            torch.tensor(df['document'].to_list(), dtype=torch.long),\n",
    "            torch.tensor(df['label'].to_list(), dtype=torch.long),\n",
    "        )\n",
    "        return DataLoader(\n",
    "            dataset,\n",
    "            batch_size=self.hparams.batch_size * 1 if not self.hparams.tpu_cores else self.hparams.tpu_cores,\n",
    "            shuffle=shuffle,\n",
    "            num_workers=self.hparams.cpu_workers,\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self.dataloader(self.hparams.train_data_path, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.dataloader(self.hparams.val_data_path, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saving-female",
   "metadata": {},
   "source": [
    "### 3.1 모델 체크포인트 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "angry-justice",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filename='./model/epoch{epoch}-val_acc{val_acc:.4f}',\n",
    "    monitor='val_acc',\n",
    "    save_top_k=3,\n",
    "    mode='max',\n",
    "    auto_insert_metric_name=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respiratory-majority",
   "metadata": {},
   "source": [
    "## 4. 모델 학습 및 검증 시작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "synthetic-arcade",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch Ver 1.9.0+cu111\n",
      "Fix Seed: 42\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f784e1eb788147a8a5f98a82937e9dd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/504 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8908717f14f345769352cb373d31bbc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/475M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at beomi/KcELECTRA-base were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at beomi/KcELECTRA-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e870d82e513c4461a83c0cf438f0a0be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/288 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a6040e587eb44a6aac5e8eb6b472f21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/387k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fce9c0c2cd1241ce921fd6b95123c35e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: Start Training ::\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/optimizers.py:227: RuntimeWarning: Found unsupported keys in the optimizer configuration: {'scheduler'}\n",
      "  rank_zero_warn(f\"Found unsupported keys in the optimizer configuration: {set(extra_keys)}\", RuntimeWarning)\n",
      "\n",
      "  | Name    | Type                             | Params\n",
      "-------------------------------------------------------------\n",
      "0 | clsfier | ElectraForSequenceClassification | 124 M \n",
      "-------------------------------------------------------------\n",
      "124 M     Trainable params\n",
      "0         Non-trainable params\n",
      "124 M     Total params\n",
      "249.093   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef0e46caef3948738669612a9bdd7082",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 VAL] Loss: 0.2270049899816513, Acc: 0.90974, Prec: 0.8925368597051223, Rec: 0.9330632026377468, F1: 0.9123502107246207\n",
      "[Epoch 0 TRAIN] Loss: 0.2797822952270508, Acc: 0.8796866666666666, Prec: 0.8756185335132703, Rec: 0.8844534726769749, F1: 0.8800138289597033\n"
     ]
    }
   ],
   "source": [
    "print(\"Using PyTorch Ver\", torch.__version__)\n",
    "print(\"Fix Seed:\", args['random_seed'])\n",
    "seed_everything(args['random_seed'])\n",
    "model = Model(**args)\n",
    "\n",
    "print(\":: Start Training ::\")\n",
    "trainer = Trainer(\n",
    "    callbacks=[checkpoint_callback],\n",
    "    max_epochs=args['epochs'],\n",
    "    fast_dev_run=args['test_mode'],\n",
    "    num_sanity_val_steps=None if args['test_mode'] else 0,\n",
    "    # For GPU Setup\n",
    "    deterministic=torch.cuda.is_available(),\n",
    "    gpus=[0] if torch.cuda.is_available() else None,  # 0번 idx GPU  사용\n",
    "    precision=16 if args['fp16'] and torch.cuda.is_available() else 32,\n",
    "    # For TPU Setup\n",
    "    # tpu_cores=args['tpu_cores'] if args['tpu_cores'] else None,\n",
    ")\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "restricted-research",
   "metadata": {},
   "source": [
    "## 5. 모델 추론 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "optical-arcade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from glob import glob\n",
    "\n",
    "latest_ckpt = sorted(glob('./lightning_logs/version_0/checkpoints/*.ckpt'))\n",
    "latest_ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "public-rotation",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model.load_from_checkpoint(latest_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "alternative-mayor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(x):\n",
    "    return torch.softmax(\n",
    "        model(**model.tokenizer(x, return_tensors='pt')\n",
    "    ).logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "parallel-image",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8658, 0.1342]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infer('이 영화 노잼 ㅡㅡ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "orange-treatment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1781, 0.8219]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infer('이  영화  꿀잼! 완존  추천요  ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "lovely-forum",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"./nsmc_data/ratings_test.txt\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fantastic-charles",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = test[\"document\"].apply(lambda x: infer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinguished-instrument",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
