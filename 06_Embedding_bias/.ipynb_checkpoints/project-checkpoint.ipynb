{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "crazy-relative",
   "metadata": {},
   "source": [
    "## 데이터의 편향성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amino-tattoo",
   "metadata": {},
   "source": [
    "데이터들이 어떠한 특성에 쏠려있으면 그 데이터는 편향성을 띄고 있다고 부르며, 편향된 데이터를 학습한 모델 또한 그 특성에 집중하게 되면서 편향성을 가지는 모델이 만들어지게 됩니다.\n",
    "\n",
    "데이터 편향성이 문제가 되는 사례로는 여러가지가 있지만 그 중에서 대표적인 사례로 마이크로소프트의 '테이'와 스케터랩의 '이루다'라는 특정 페르소나를 가진 오픈 도메인 형식의 인공지능 챗봇이 일부 사용자들의 혐오 발언을 새로운 학습 데이터로 학습을 하며 다른 사용자에게 불쾌감을 주는 문제가 발생하였고 결국에는 서비스 종료를 하게 되었습니다.\n",
    "\n",
    "사례를 통하여 데이터의 편향성을 파악하는 것이 중요하다는 것을 알 수가 있었는데 해당 노트북에서 진행될 내용은 텍스트의 임베딩에 숨어있는 편향성을 측정하는 방법으로 Word Embedding Association Test(WEAT)을 소개하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affecting-symphony",
   "metadata": {},
   "source": [
    "## 편향의 정량적 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "olive-jesus",
   "metadata": {},
   "source": [
    "예를 들어서, 남자가 과학에, 여자가 예술에 가깝다는 편향을 임베딩 벡터상에서 정의를 해보겠습니다.\n",
    "\n",
    "과학와 예술이 모두 젠더 중립적이라면 임베딩 벡터상에서 과학과 예술이 각각 남자와 여자간의 거리가 동일해야 합니다. \n",
    "\n",
    "하지만 만약 과학이 남자와 여자라는 두 단어와의 거리가 동일하지 않고 남자와의 거리가 더 가깝다면 과학이라는 단어는 남자라는 단어에 편향되어 있다고 할 수 있습니다.\n",
    "\n",
    "여기에 추가적으로 과학이라는 단어가 편향되었다고 좀 더 객관적인 의견을 얻기 위해서 과학이라는 단어를 잘 대표하는 단어들을 여러개 골라서 단어 셋을 만들고 단어 셋에 속한 모든 단어들의 편향성을 계산한 평균을 수치화해주겠습니다.\n",
    "\n",
    "여기서 과학은 target이되고 과학을 대표하는 단어들을 attribute가 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "junior-drove",
   "metadata": {},
   "source": [
    "WEAT score는 아래와 같습니다.\n",
    "\n",
    "$score_{weat} = \\frac{mena_}{}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documentary-steal",
   "metadata": {},
   "source": [
    "사전학습 WEAT score\n",
    "\n",
    "https://drive.google.com/u/0/uc?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM&export=download\n",
    "\n",
    "https://www.inflearn.com/questions/199313"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "checked-composite",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "devoted-things",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(i, j):\n",
    "    return dot(i, j.T)/(norm(i)*norm(j))\n",
    "\n",
    "def s(w, A, B):\n",
    "    c_a = cos_sim(w, A)\n",
    "    c_b = cos_sim(w, B)\n",
    "    mean_A = np.mean(c_a, axis=-1)\n",
    "    mean_B = np.mean(c_b, axis=-1)\n",
    "    return mean_A - mean_B #, c_a, c_b\n",
    "\n",
    "# print(s(target_X['장미'], A, B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "responsible-current",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weat_score(X, Y, A, B):\n",
    "    \n",
    "    s_X = s(X, A, B)\n",
    "    s_Y = s(Y, A, B)\n",
    "\n",
    "    mean_X = np.mean(s_X)\n",
    "    mean_Y = np.mean(s_Y)\n",
    "    \n",
    "    std_dev = np.std(np.concatenate([s_X, s_Y], axis=0))\n",
    "    \n",
    "    return  (mean_X-mean_Y)/std_dev\n",
    "\n",
    "# print(round(weat_score(X, Y, A, B), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patent-receipt",
   "metadata": {},
   "source": [
    "## 직접 만든 임베딩 벡터에 WEAT 적용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "pressed-employee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 젊은 시절 상처한 한 아버지(박인환 분), 고모(신신애 분)와 함께 살고 있다.\n",
      " 자연의 소리를 채집해 틀어주는 라디오 프로그램을 준비하는 은수는 상우와 녹음 여행을 떠난다.\n",
      " 너무 쉽게 사랑에 빠진 두 사람... 상우는 주체할 수 없을 정도로 그녀에게 빨려든다.\n",
      " 이혼 경험이 있는 은수는 상우에게 결혼할 생각이 없다며 부담스러운 표정을 내비친다.\n",
      " 영원히 변할 것 같지 않던 사랑이 변하고, 그 사실을 받아들이지 못하는 상우는 어찌 할 바를 모른다.\n",
      "유사 이래 연령, 성별, 빈부의 차이와 정치적인 입장을 불문하고 일거에 국민을 통합해 온 '애국심'이라는 성역에 일침을 가하는 다큐멘터리. 재작년 전국 민족민주 유가족협의회의 장기농성을 다룬 인상적인 다큐멘터리 <민들레>를 만들었던 독립영화집단 '빨간 눈사람'이 우리 사회 구석구석을 발빠르게 돌아다니며 애국심과 민족주의가 강요되는 현장을 발굴하여 카메라에 담았다. 박홍 서강대 명예총장, 이도형 '한국논단' 발행인, 축구해설자 신문선, 홍세화, 박노해 등 사회 각계의 '스타'들이 등장해 저마다의 확고한 신념을 성토한다. 감독 이경순과 최하동하는 이 작품을 위해 3년간 백여 명을 인터뷰했다고 한다. 2001 올해의 독립영화상 수상.\n",
      "엽기적인 살인사건이 발생한 장소를 관광하는 투어팀. 그 팀에서 관광객들은 살인사건과 관련하여 히스테리컬한 반응을 보이는데 과연 이들의 정체는? (Tourists see whrer a murder take place. They respond hysterically to the murder…what are they?)\n",
      "착하지만 엉뚱한 태희(배두나 분), 예쁜 깍쟁이 혜주(이요원 분), 그림을 잘 그리는 지영(옥지영 분), 명랑한 쌍둥이 비류(이은실 분)와 온조(이은주 분)는 단짝친구들. 늘 함께였던 그들이지만 스무 살이 되면서 길이 달라진다. 증권회사에 입사한 혜주는 성공한 커리어우먼의 야심을 키우고 미술에 재능이 있는 지영은 유학을 꿈꾼다. 한편 태희는 봉사활동에서 알게 된 뇌성마비 시인을 좋아하는데...\n",
      "인도 등 아시아 식민지에 처음 발을 디딘 뒤 여행하고 “경영”한 이들은 과연 누구였을까? 과거의 이미지들은, 이민과 인종 문제, ‘오리엔탈리즘’이 격렬히 충돌하고 있는 현재와 강력하게 공명한다.\n",
      "홀로 살아가는 미국 할머니와 한국 할머니의 이야기. 공원에서 가끔 마주치게 되는 그들은 비록 언어 소통의 어려움을 겪지만 시간이 흘러감에 따라 서로 가까워져 그들의 외로움과 우정을 공유하게 된다. 겨울이 지나고 봄이 왔을 때 길가의 민들레 홀씨는 삶의 이치를 말해주듯 한 할머니의 주위를 맴돈다. (Two elderly widows, an American and a Korean, frequent the same park in Philadelphia and attempt a friendship, though the Korean widow speaks no English. Driven by loneliness and a spark of hope, they persevere within the limits of body language, and the outcome poses a question of life as fundamental as a flower.)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "with open('./data/synopsis.txt', 'r') as file:\n",
    "    for i in range(10):\n",
    "        aa = file.readline()\n",
    "        print(file.readline(), end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "material-sunday",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "from packages.utils import text_preprocessing\n",
    "\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aerial-kitty",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_en(x):\n",
    "    x = re.sub(\"[^ㄱ-ㅎ가-힣]+\", \" \", x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "suffering-affiliation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "걸린 시간 : 0.2567276954650879\n"
     ]
    }
   ],
   "source": [
    "mecab = Mecab()\n",
    "data = []\n",
    "\n",
    "start = time.time()\n",
    "with open('./data/synopsis.txt', 'r') as file:\n",
    "    while True:\n",
    "        \n",
    "        line = file.readline()\n",
    "        line = remove_en(line)\n",
    "        line = text_preprocessing(line)\n",
    "        \n",
    "        if not line: \n",
    "            break\n",
    "            \n",
    "        words = mecab.nouns(line)\n",
    "        data += [words]\n",
    "        \n",
    "end = time.time()\n",
    "print(f\"걸린 시간 : {end - start}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adopted-priority",
   "metadata": {},
   "source": [
    "## word2vec 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "plain-bachelor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('작품', 0.8821832537651062),\n",
       " ('감독', 0.8482502698898315),\n",
       " ('출연', 0.8458046913146973),\n",
       " ('소설', 0.8401564955711365),\n",
       " ('형식', 0.8329927921295166),\n",
       " ('코미디', 0.8307586908340454),\n",
       " ('연기', 0.8305863738059998),\n",
       " ('예술', 0.8280542492866516),\n",
       " ('독특', 0.8273999691009521),\n",
       " ('관객', 0.8239632248878479)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# tokenized에 담긴 데이터를 가지고 나만의 Word2Vec을 생성합니다. (Gensim 4.0 기준)\n",
    "model = Word2Vec(data, vector_size=100, window=5, min_count=3, sg=1)  \n",
    "model.wv.most_similar(positive=['영화'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "royal-center",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['사운드', '엔지니어', '상우', '유지태', '분', '치매', '할머니', '백', '성희', '분']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elementary-selling",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform([art, gen])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enhanced-teaching",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modern-popularity",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supported-wrapping",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "art_txt = 'synopsis_art.txt'\n",
    "gen_txt = 'synopsis_gen.txt'\n",
    "\n",
    "def read_token(file_name):\n",
    "    okt = Okt()\n",
    "    result = []\n",
    "    with open('./aiffel/Going_Deeper_Project/06_Embedding_bias/data/'+file_name, 'r') as fread: \n",
    "        print(file_name, '파일을 읽고 있습니다.')\n",
    "        while True:\n",
    "            line = fread.readline() \n",
    "            if not line: break \n",
    "            tokenlist = okt.pos(line, stem=True, norm=True) \n",
    "            for word in tokenlist:\n",
    "                if word[1] in [\"Noun\"]:#, \"Adjective\", \"Verb\"]:\n",
    "                    result.append((word[0])) \n",
    "    return ' '.join(result)\n",
    "\n",
    "print(\"슝~\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revolutionary-location",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2개의 파일을 처리하는데 10분 가량 걸립니다. \n",
    "art = read_token(art_txt)\n",
    "gen = read_token(gen_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worthy-language",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform([art, gen])\n",
    "\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charged-personality",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vectorizer.vocabulary_['영화'])\n",
    "print(vectorizer.get_feature_names()[23976])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlimited-muslim",
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = X[0].tocoo()   # art를 TF-IDF로 표현한 sparse matrix를 가져옵니다. \n",
    "m2 = X[1].tocoo()   # gen을 TF-IDF로 표현한 sparse matrix를 가져옵니다. \n",
    "\n",
    "w1 = [[i, j] for i, j in zip(m1.col, m1.data)]\n",
    "w2 = [[i, j] for i, j in zip(m2.col, m2.data)]\n",
    "\n",
    "w1.sort(key=lambda x: x[1], reverse=True)   #art를 구성하는 단어들을 TF-IDF가 높은 순으로 정렬합니다. \n",
    "w2.sort(key=lambda x: x[1], reverse=True)   #gen을 구성하는 단어들을 TF-IDF가 높은 순으로 정렬합니다. \n",
    "\n",
    "print('예술영화를 대표하는 단어들:')\n",
    "for i in range(100):\n",
    "    print(vectorizer.get_feature_names()[w1[i][0]], end=', ')\n",
    "\n",
    "print('\\n')\n",
    "    \n",
    "print('일반영화를 대표하는 단어들:')\n",
    "for i in range(100):\n",
    "    print(vectorizer.get_feature_names()[w2[i][0]], end=', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supreme-battery",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 15\n",
    "w1_, w2_ = [], []\n",
    "for i in range(100):\n",
    "    w1_.append(vectorizer.get_feature_names()[w1[i][0]])\n",
    "    w2_.append(vectorizer.get_feature_names()[w2[i][0]])\n",
    "\n",
    "# w1에만 있고 w2에는 없는, 예술영화를 잘 대표하는 단어를 15개 추출한다.\n",
    "target_art, target_gen = [], []\n",
    "for i in range(100):\n",
    "    if (w1_[i] not in w2_) and (w1_[i] in model.wv): target_art.append(w1_[i])\n",
    "    if len(target_art) == n: break \n",
    "\n",
    "# w2에만 있고 w1에는 없는, 일반영화를 잘 대표하는 단어를 15개 추출한다.\n",
    "for i in range(100):\n",
    "    if (w2_[i] not in w1_) and (w2_[i] in model.wv): target_gen.append(w2_[i])\n",
    "    if len(target_gen) == n: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geological-desktop",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(target_art)\n",
    "print(target_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distributed-medium",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "random-receipt",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apart-bottom",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resident-contractor",
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_txt = ['synopsis_drama.txt', 'synopsis_romance.txt', 'synopsis_action.txt', 'synopsis_comedy.txt', 'synopsis_war.txt', 'synopsis_horror.txt']\n",
    "genre_name = ['드라마', '멜로로맨스', '액션', '코미디', '전쟁', '공포(호러)']\n",
    "\n",
    "print(\"슝~\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "younger-blank",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 약 10분정도 걸립니다.\n",
    "genre = []\n",
    "for file_name in genre_txt:\n",
    "    genre.append(read_token(file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "textile-respect",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(genre)\n",
    "\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "political-maximum",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = [X[i].tocoo() for i in range(X.shape[0])]\n",
    "\n",
    "w = [[[i, j] for i, j in zip(mm.col, mm.data)] for mm in m]\n",
    "\n",
    "for i in range(len(w)):\n",
    "    w[i].sort(key=lambda x: x[1], reverse=True)\n",
    "attributes = []\n",
    "for i in range(len(w)):\n",
    "    print(genre_name[i], end=': ')\n",
    "    attr = []\n",
    "    j = 0\n",
    "    while (len(attr) < 15):\n",
    "        if vectorizer.get_feature_names()[w[i][j][0]] in model.wv:\n",
    "            attr.append(vectorizer.get_feature_names()[w[i][j][0]])\n",
    "            print(vectorizer.get_feature_names()[w[i][j][0]], end=', ')\n",
    "        j += 1\n",
    "    attributes.append(attr)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limited-suicide",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "danish-universal",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relevant-inquiry",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twenty-service",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = [[0 for _ in range(len(genre_name))] for _ in range(len(genre_name))]\n",
    "print(\"슝~\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binding-majority",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([model.wv[word] for word in target_art])\n",
    "Y = np.array([model.wv[word] for word in target_gen])\n",
    "\n",
    "for i in range(len(genre_name)-1):\n",
    "    for j in range(i+1, len(genre_name)):\n",
    "        A = np.array([model.wv[word] for word in attributes[i]])\n",
    "        B = np.array([model.wv[word] for word in attributes[j]])\n",
    "        matrix[i][j] = weat_score(X, Y, A, B)\n",
    "\n",
    "print(\"슝~\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amino-diana",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(genre_name)-1):\n",
    "    for j in range(i+1, len(genre_name)):\n",
    "        print(genre_name[i], genre_name[j],matrix[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agricultural-lancaster",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amber-maple",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monetary-recall",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brutal-decrease",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np; \n",
    "import seaborn as sns; \n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# 한글 지원 폰트\n",
    "sns.set(font='NanumGothic')\n",
    "\n",
    "# 마이너스 부호 \n",
    "\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "ax = sns.heatmap(matrix, xticklabels=genre_name, yticklabels=genre_name, annot=True,  cmap='RdYlGn_r')\n",
    "ax"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
