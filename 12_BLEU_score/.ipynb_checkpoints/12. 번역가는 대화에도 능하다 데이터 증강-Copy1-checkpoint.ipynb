{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "finnish-stretch",
   "metadata": {},
   "source": [
    "# 1. 서론"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final-realtor",
   "metadata": {},
   "source": [
    "#### 기본적으로 사용되는 패키지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "announced-parish",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from packages import utils, tokenizer # 사용자 지정 패키지입니다.\n",
    "from konlpy.tag import Mecab\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm import tqdm\n",
    "import gensim\n",
    "from tqdm import tqdm_notebook \n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contrary-exclusive",
   "metadata": {},
   "source": [
    "#### matplotlib 한글 깨짐 해결 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "geological-convertible",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한글 지원 폰트\n",
    "sns.set(font='NanumGothic')\n",
    "\n",
    "# 마이너스 부호 \n",
    "\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "usual-writer",
   "metadata": {},
   "source": [
    "# 2. 본론"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriented-crash",
   "metadata": {},
   "source": [
    "## 2.1 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "three-height",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 개수 : 11823\n",
      "컬럼 개수 : 3\n",
      "컬럼명 : Index(['Q', 'A', 'label'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "dic = utils.load_data_all(\"./data/ChatbotData.csv\")\n",
    "file_name = list(dic.keys())[0]\n",
    "data = dic[file_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaptive-distribution",
   "metadata": {},
   "source": [
    "## 2.2 데이터 전처리\n",
    "- 특수문자 제거\n",
    "- 결측 데이터 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eleven-rogers",
   "metadata": {},
   "source": [
    "중복 데이터까지 제거하게 되면 너무 많은 데이터의 소실이 생기기 때문에 중복 데이터 제거는 하지 않겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affecting-insulation",
   "metadata": {},
   "source": [
    "### 2.2.1 특수문자 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "detected-history",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"pre_Q\"] = data[\"Q\"].apply(lambda x: utils.text_prep(x))\n",
    "data[\"pre_A\"] = data[\"A\"].apply(lambda x: utils.text_prep(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finished-macro",
   "metadata": {},
   "source": [
    "### 2.2.2 결측 데이터 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "warming-stack",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = utils.remove_nan(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stone-problem",
   "metadata": {},
   "source": [
    "### 2.2.3 데이터 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "experimental-happiness",
   "metadata": {},
   "outputs": [],
   "source": [
    "mecab = Mecab()\n",
    "data[\"pre_Q\"] = data[\"pre_Q\"].apply(lambda x: mecab.morphs(x))\n",
    "data[\"pre_A\"] = data[\"pre_A\"].apply(lambda x: mecab.morphs(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "remarkable-village",
   "metadata": {},
   "source": [
    "## 2.3 데이터 증강"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "wooden-inquiry",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "      <th>pre_Q</th>\n",
       "      <th>pre_A</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "      <td>0</td>\n",
       "      <td>[12, 시, 땡, !]</td>\n",
       "      <td>[하루, 가, 또, 가, 네요]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 지망, 학교, 떨어졌, 어]</td>\n",
       "      <td>[위로, 해, 드립니다]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "      <td>[3, 박, 4, 일, 놀, 러, 가, 고, 싶, 다]</td>\n",
       "      <td>[여행, 은, 언제나, 좋, 죠]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "      <td>[3, 박, 4, 일, 정도, 놀, 러, 가, 고, 싶, 다]</td>\n",
       "      <td>[여행, 은, 언제나, 좋, 죠]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "      <td>0</td>\n",
       "      <td>[ppl, 심하, 네]</td>\n",
       "      <td>[눈살, 이, 찌푸려, 지, 죠]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Q            A  label                               pre_Q  \\\n",
       "0           12시 땡!   하루가 또 가네요.      0                       [12, 시, 땡, !]   \n",
       "1      1지망 학교 떨어졌어    위로해 드립니다.      0                 [1, 지망, 학교, 떨어졌, 어]   \n",
       "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0      [3, 박, 4, 일, 놀, 러, 가, 고, 싶, 다]   \n",
       "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0  [3, 박, 4, 일, 정도, 놀, 러, 가, 고, 싶, 다]   \n",
       "4          PPL 심하네   눈살이 찌푸려지죠.      0                        [ppl, 심하, 네]   \n",
       "\n",
       "                pre_A  \n",
       "0   [하루, 가, 또, 가, 네요]  \n",
       "1       [위로, 해, 드립니다]  \n",
       "2  [여행, 은, 언제나, 좋, 죠]  \n",
       "3  [여행, 은, 언제나, 좋, 죠]  \n",
       "4  [눈살, 이, 찌푸려, 지, 죠]  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adopted-kitchen",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = gensim.models.Word2Vec.load(\"./data/ko.bin\")\n",
    "copy_Q, copy_A = data[\"pre_Q\"].copy(), data[\"pre_A\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "loving-fruit",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augmentation(data, copy_Q, copy_A):\n",
    "    augment_list = []\n",
    "    for Q, A in tqdm(zip(copy_Q, copy_A)):\n",
    "        cp_Q = Q.copy()\n",
    "        random_word = np.random.choice(cp_Q, 1)[0]\n",
    "        idx = cp_Q.index(random_word)\n",
    "        try:\n",
    "            cp_Q[idx] = word2vec.wv.most_similar(random_word)[0][0]\n",
    "        except:\n",
    "            continue\n",
    "        augment_list.append([cp_Q, A])\n",
    "\n",
    "    aug_dataset = pd.DataFrame(augment_list, columns = [\"pre_Q\", \"pre_A\"])\n",
    "    data = pd.concat([data, aug_dataset])\n",
    "    print(f\"데이터의 개수 : {data.shape[0]}\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "included-preservation",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11823it [00:25, 470.02it/s]\n",
      "56it [00:00, 511.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터의 개수 : 21779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11823it [00:21, 542.87it/s]\n",
      "62it [00:00, 616.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터의 개수 : 31740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11823it [00:21, 539.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터의 개수 : 41728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 데이터 개수 3배로 늘리기\n",
    "for _ in range(3):\n",
    "    data = data_augmentation(data, copy_Q, copy_A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chinese-attribute",
   "metadata": {},
   "source": [
    "데이터의 개수를 data augmentation 방법중에서 Lexical Substitution을 적용하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "synthetic-template",
   "metadata": {},
   "source": [
    "#### 문장별 토큰화된 단어 개수 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fifty-highlight",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "data[\"len_Q\"] = data[\"pre_Q\"].apply(lambda x: len(x))\n",
    "count_len_Q = dict(Counter(data[\"len_Q\"]))\n",
    "sort_len_Q = sorted(count_len_Q.items(), key = lambda x : x[0])\n",
    "sort_len_Q = np.array(sort_len_Q)\n",
    "\n",
    "data[\"len_A\"] = data[\"pre_A\"].apply(lambda x: len(x))\n",
    "count_len_A = dict(Counter(data[\"len_A\"]))\n",
    "sort_len_A = sorted(count_len_A.items(), key = lambda x : x[0])\n",
    "sort_len_A = np.array(sort_len_A)\n",
    "\n",
    "\n",
    "plt.bar(sort_len_A[:,0], sort_len_A[:,1], label = \"A\")\n",
    "plt.bar(sort_len_Q[:,0], sort_len_Q[:,1], label = \"Q\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"토큰의 개수 및 문장의 길이\")\n",
    "plt.ylabel(\"문장의 개수\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assured-underwear",
   "metadata": {},
   "source": [
    "#### 문장의 길이 20 이상인 데이터 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "terminal-retrieval",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "      <th>pre_Q</th>\n",
       "      <th>pre_A</th>\n",
       "      <th>len_Q</th>\n",
       "      <th>len_A</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>가장 확실한 건 뭘까?</td>\n",
       "      <td>가장 확실한 시간은 오늘이에요. 어제와 내일을 놓고 고민하느라 시간을 낭비하지 마세요.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[가장, 확실, 한, 건, 뭘까, ?]</td>\n",
       "      <td>[가장, 확실, 한, 시간, 은, 오늘, 이, 에요, 어제, 와, 내일, 을, 놓,...</td>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>가족 있어?</td>\n",
       "      <td>저를 만들어 준 사람을 부모님, 저랑 이야기해 주는 사람을 친구로 생각하고 있어요</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[가족, 있, 어, ?]</td>\n",
       "      <td>[저, 를, 만들, 어, 준, 사람, 을, 부모, 님, ,, 저, 랑, 이야기, 해...</td>\n",
       "      <td>4</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>가족관계 알려 줘</td>\n",
       "      <td>저를 만들어 준 사람을 부모님, 저랑 이야기해 주는 사람을 친구로 생각하고 있어요</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[가족, 관계, 알려, 줘]</td>\n",
       "      <td>[저, 를, 만들, 어, 준, 사람, 을, 부모, 님, ,, 저, 랑, 이야기, 해...</td>\n",
       "      <td>4</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>가족이 누구야?</td>\n",
       "      <td>저를 만들어 준 사람을 부모님, 저랑 이야기해 주는 사람을 친구로 생각하고 있어요</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[가족, 이, 누구, 야, ?]</td>\n",
       "      <td>[저, 를, 만들, 어, 준, 사람, 을, 부모, 님, ,, 저, 랑, 이야기, 해...</td>\n",
       "      <td>5</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588</th>\n",
       "      <td>나는 왜 이렇게 태어났을까?</td>\n",
       "      <td>서로 다르게 태어난 이유는 저마다의 목소리를 내기 위해서예요. 자신의 목소리를 들어...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[나, 는, 왜, 이렇게, 태어났, 을까, ?]</td>\n",
       "      <td>[서로, 다르, 게, 태어난, 이유, 는, 저마다, 의, 목소리, 를, 내, 기, ...</td>\n",
       "      <td>7</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9813</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[짝사랑, 중, 인데, 그, 애, 는, 저, 를, 잘, 알, 지, 못하, 는데, 연...</td>\n",
       "      <td>[갑자기, 연락, 하, 면, 놀라, 겠죠]</td>\n",
       "      <td>21</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9929</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[하루, 종일, 같이, 있, 고, 싶, 어서]</td>\n",
       "      <td>[서로, 떨어져서, 그리워할, 시간, 을, 적당히, 만들, 어, 주, 는, 게, 관...</td>\n",
       "      <td>7</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9930</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[하루, 종일, 붙, 어서, 있, 고, 싶, 어]</td>\n",
       "      <td>[뭐, 든, 함께, 하, 려는, 것, 도, 좋, 겠, 지만, 오래, 붙, 어, 잇,...</td>\n",
       "      <td>8</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9976</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[확실히, 좋, 아, 하, 는, 데, 도, 관심, 없, 는, 거, 티, 안, 내, ...</td>\n",
       "      <td>[많이, 있, 어요]</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9981</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[회식, 중, 이, 라고, 하, 으며, 연락, 이, 안, 돼]</td>\n",
       "      <td>[정신, 없이, 바쁠, 지도, 몰라요, 조금, 만, 더, 기다려, 보, 고, 물, ...</td>\n",
       "      <td>10</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>401 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Q                                                  A  \\\n",
       "22       가장 확실한 건 뭘까?   가장 확실한 시간은 오늘이에요. 어제와 내일을 놓고 고민하느라 시간을 낭비하지 마세요.   \n",
       "26             가족 있어?      저를 만들어 준 사람을 부모님, 저랑 이야기해 주는 사람을 친구로 생각하고 있어요   \n",
       "27          가족관계 알려 줘      저를 만들어 준 사람을 부모님, 저랑 이야기해 주는 사람을 친구로 생각하고 있어요   \n",
       "35           가족이 누구야?      저를 만들어 준 사람을 부모님, 저랑 이야기해 주는 사람을 친구로 생각하고 있어요   \n",
       "588   나는 왜 이렇게 태어났을까?  서로 다르게 태어난 이유는 저마다의 목소리를 내기 위해서예요. 자신의 목소리를 들어...   \n",
       "...               ...                                                ...   \n",
       "9813              NaN                                                NaN   \n",
       "9929              NaN                                                NaN   \n",
       "9930              NaN                                                NaN   \n",
       "9976              NaN                                                NaN   \n",
       "9981              NaN                                                NaN   \n",
       "\n",
       "      label                                              pre_Q  \\\n",
       "22      0.0                              [가장, 확실, 한, 건, 뭘까, ?]   \n",
       "26      0.0                                      [가족, 있, 어, ?]   \n",
       "27      0.0                                    [가족, 관계, 알려, 줘]   \n",
       "35      0.0                                  [가족, 이, 누구, 야, ?]   \n",
       "588     0.0                         [나, 는, 왜, 이렇게, 태어났, 을까, ?]   \n",
       "...     ...                                                ...   \n",
       "9813    NaN  [짝사랑, 중, 인데, 그, 애, 는, 저, 를, 잘, 알, 지, 못하, 는데, 연...   \n",
       "9929    NaN                          [하루, 종일, 같이, 있, 고, 싶, 어서]   \n",
       "9930    NaN                        [하루, 종일, 붙, 어서, 있, 고, 싶, 어]   \n",
       "9976    NaN  [확실히, 좋, 아, 하, 는, 데, 도, 관심, 없, 는, 거, 티, 안, 내, ...   \n",
       "9981    NaN                 [회식, 중, 이, 라고, 하, 으며, 연락, 이, 안, 돼]   \n",
       "\n",
       "                                                  pre_A  len_Q  len_A  \n",
       "22    [가장, 확실, 한, 시간, 은, 오늘, 이, 에요, 어제, 와, 내일, 을, 놓,...      6     23  \n",
       "26    [저, 를, 만들, 어, 준, 사람, 을, 부모, 님, ,, 저, 랑, 이야기, 해...      4     25  \n",
       "27    [저, 를, 만들, 어, 준, 사람, 을, 부모, 님, ,, 저, 랑, 이야기, 해...      4     25  \n",
       "35    [저, 를, 만들, 어, 준, 사람, 을, 부모, 님, ,, 저, 랑, 이야기, 해...      5     25  \n",
       "588   [서로, 다르, 게, 태어난, 이유, 는, 저마다, 의, 목소리, 를, 내, 기, ...      7     22  \n",
       "...                                                 ...    ...    ...  \n",
       "9813                            [갑자기, 연락, 하, 면, 놀라, 겠죠]     21      6  \n",
       "9929  [서로, 떨어져서, 그리워할, 시간, 을, 적당히, 만들, 어, 주, 는, 게, 관...      7     24  \n",
       "9930  [뭐, 든, 함께, 하, 려는, 것, 도, 좋, 겠, 지만, 오래, 붙, 어, 잇,...      8     28  \n",
       "9976                                        [많이, 있, 어요]     32      3  \n",
       "9981  [정신, 없이, 바쁠, 지도, 몰라요, 조금, 만, 더, 기다려, 보, 고, 물, ...     10     21  \n",
       "\n",
       "[401 rows x 7 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "more_length = 20\n",
    "data.loc[(data[\"len_Q\"] > more_length) | (data[\"len_A\"] > more_length), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "passing-video",
   "metadata": {},
   "source": [
    "#### 문장의 길이 2 미만인 데이터 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "million-invalid",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "      <th>pre_Q</th>\n",
       "      <th>pre_A</th>\n",
       "      <th>len_Q</th>\n",
       "      <th>len_A</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>건방져</td>\n",
       "      <td>기분이 나쁘셨나봐요.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[건방져]</td>\n",
       "      <td>[기분, 이, 나쁘, 셨, 나, 봐요]</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>겁난다</td>\n",
       "      <td>용기 내보세요.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[겁난다]</td>\n",
       "      <td>[용기, 내보, 세요]</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>결혼하면 좋아?</td>\n",
       "      <td>해봐요.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[결혼, 하, 면, 좋, 아, ?]</td>\n",
       "      <td>[해봐요]</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>결혼해야 하나</td>\n",
       "      <td>해봐요.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[결혼, 해야, 하나]</td>\n",
       "      <td>[해봐요]</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>굿모닝</td>\n",
       "      <td>좋은 아침이에요.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[굿모닝]</td>\n",
       "      <td>[좋, 은, 아침, 이, 에요]</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7504</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[회개]</td>\n",
       "      <td>[성공, 하, 길, 바랄, 게요]</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[에드, 생각, 이, 났, 어]</td>\n",
       "      <td>[어머나]</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8275</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[사랑, 꾼들]</td>\n",
       "      <td>[부러워요]</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9432</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[좋, 아, 하, 는, 애, 한테, 찌르, 적기, 눌러, 버림]</td>\n",
       "      <td>[아이쿠]</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9604</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[짝, 남, 이, 내, 놀드, 좋, 아, 하, 는, 거, 알, 아, 버림]</td>\n",
       "      <td>[이런]</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>387 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Q            A  label                                      pre_Q  \\\n",
       "118        건방져  기분이 나쁘셨나봐요.    0.0                                      [건방져]   \n",
       "123        겁난다     용기 내보세요.    0.0                                      [겁난다]   \n",
       "157   결혼하면 좋아?         해봐요.    0.0                        [결혼, 하, 면, 좋, 아, ?]   \n",
       "165    결혼해야 하나         해봐요.    0.0                               [결혼, 해야, 하나]   \n",
       "293        굿모닝    좋은 아침이에요.    0.0                                      [굿모닝]   \n",
       "...        ...          ...    ...                                        ...   \n",
       "7504       NaN          NaN    NaN                                       [회개]   \n",
       "7997       NaN          NaN    NaN                          [에드, 생각, 이, 났, 어]   \n",
       "8275       NaN          NaN    NaN                                   [사랑, 꾼들]   \n",
       "9432       NaN          NaN    NaN        [좋, 아, 하, 는, 애, 한테, 찌르, 적기, 눌러, 버림]   \n",
       "9604       NaN          NaN    NaN  [짝, 남, 이, 내, 놀드, 좋, 아, 하, 는, 거, 알, 아, 버림]   \n",
       "\n",
       "                      pre_A  len_Q  len_A  \n",
       "118   [기분, 이, 나쁘, 셨, 나, 봐요]      1      6  \n",
       "123            [용기, 내보, 세요]      1      3  \n",
       "157                   [해봐요]      6      1  \n",
       "165                   [해봐요]      3      1  \n",
       "293       [좋, 은, 아침, 이, 에요]      1      5  \n",
       "...                     ...    ...    ...  \n",
       "7504     [성공, 하, 길, 바랄, 게요]      1      5  \n",
       "7997                  [어머나]      5      1  \n",
       "8275                 [부러워요]      2      1  \n",
       "9432                  [아이쿠]     10      1  \n",
       "9604                   [이런]     13      1  \n",
       "\n",
       "[387 rows x 7 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "less_length = 2\n",
    "data.loc[(data[\"len_Q\"] < less_length) | (data[\"len_A\"] < less_length), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "familiar-continuity",
   "metadata": {},
   "source": [
    "토큰의 개수가 너무 적거나 너무 많다고 이상한 문장이 포함되어 있는 것은 아닌 것으로 판단되기 때문에 가능한 모든 데이터를 사용하도록 하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "central-episode",
   "metadata": {},
   "source": [
    "#### 토큰화 사전 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "posted-excuse",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어장 크기 : 8121\n"
     ]
    }
   ],
   "source": [
    "max_length = max(max(data[\"len_Q\"]), max(data[\"len_A\"]))\n",
    "raw_data = pd.concat([data[\"pre_Q\"], data[\"pre_A\"]])\n",
    "vocab_dictionary = tokenizer.bin_dict(raw_data)\n",
    "vocab_size = len(vocab_dictionary)\n",
    "print(f\"단어장 크기 : {vocab_size}\")\n",
    "word_index, index_word = tokenizer.word_index(vocab_dictionary, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attended-america",
   "metadata": {},
   "source": [
    "#### 인코더 디코더 모델의 입출력 형태 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "russian-procedure",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"decoder_input\"] = data[\"pre_A\"].apply(lambda x : [\"<sos>\"] + x)\n",
    "data[\"decoder_output\"] = data[\"pre_A\"].apply(lambda x : x + [\"<eos>\"])\n",
    "data[\"encoder_input\"] = data[\"pre_Q\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crude-feeling",
   "metadata": {},
   "source": [
    "#### 단어들을 사전 id값으로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "thorough-zimbabwe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "encoder_input = tokenizer.tokenization(data[\"encoder_input\"], word_index, max_length + 1)\n",
    "decoder_input = tokenizer.tokenization(data[\"decoder_input\"], word_index, max_length)\n",
    "decoder_output = tokenizer.tokenization(data[\"decoder_output\"], word_index, max_length)\n",
    "\n",
    "encoder_input.shape, decoder_input.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amateur-psychology",
   "metadata": {},
   "source": [
    "#### 변환값과 실제 단어 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "wireless-village",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 320 3525  552 1102   12    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "['1', '지망', '학교', '떨어졌', '어', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "1                [1, 지망, 학교, 떨어졌, 어]\n",
      "1               [1, 지망, 학교의, 떨어졌, 어]\n",
      "1    [3, 박, 4, 일, 놀, 러, 가, 기에, 싶, 다]\n",
      "1               [1, 지망, 학교, 떨어졌, 어서]\n",
      "Name: encoder_input, dtype: object\n",
      "\n",
      "\n",
      "[   2  546   15 1664    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "['<sos>', '위로', '해', '드립니다', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "1         [<sos>, 위로, 해, 드립니다]\n",
      "1         [<sos>, 위로, 해, 드립니다]\n",
      "1    [<sos>, 여행, 은, 언제나, 좋, 죠]\n",
      "1         [<sos>, 위로, 해, 드립니다]\n",
      "Name: decoder_input, dtype: object\n",
      "\n",
      "\n",
      "[ 546   15 1664    3    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "['위로', '해', '드립니다', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "1         [위로, 해, 드립니다, <eos>]\n",
      "1         [위로, 해, 드립니다, <eos>]\n",
      "1    [여행, 은, 언제나, 좋, 죠, <eos>]\n",
      "1         [위로, 해, 드립니다, <eos>]\n",
      "Name: decoder_output, dtype: object\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "print(encoder_input[i])\n",
    "print([index_word[k] for k in encoder_input[i]])\n",
    "print(data[\"encoder_input\"][i])\n",
    "print(\"\\n\")\n",
    "print(decoder_input[i])\n",
    "print([index_word[k] for k in decoder_input[i]])\n",
    "print(data[\"decoder_input\"][i])\n",
    "print(\"\\n\")\n",
    "print(decoder_output[i])\n",
    "print([index_word[k] for k in decoder_output[i]])\n",
    "print(data[\"decoder_output\"][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electric-blast",
   "metadata": {},
   "source": [
    "## 3.1 챗봇 모델 구축"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inclusive-problem",
   "metadata": {},
   "source": [
    "### 3.1.1 Transformer 세부 모듈 구축"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disabled-passenger",
   "metadata": {},
   "source": [
    "#### Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "established-chicago",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(pos, d_model):\n",
    "    def cal_angle(position, i):\n",
    "        return position / np.power(10000, int(i) / d_model)\n",
    "    \n",
    "    \n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, i) for i in range(d_model)]\n",
    "    \n",
    "    # (position, dimension) 생성\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(pos)])\n",
    "\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
    "\n",
    "    return sinusoid_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mediterranean-annotation",
   "metadata": {},
   "source": [
    "#### Masking Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "sublime-greek",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def generate_causality_mask(src_len, tgt_len):\n",
    "    mask = 1 - np.cumsum(np.eye(src_len, tgt_len), 0)\n",
    "    return tf.cast(mask, tf.float32)\n",
    "\n",
    "def generate_masks(src, tgt):\n",
    "    enc_mask = generate_padding_mask(src)\n",
    "    dec_mask = generate_padding_mask(tgt)\n",
    "\n",
    "    dec_causality_mask = generate_causality_mask(tgt.shape[1], tgt.shape[1])\n",
    "    dec_mask = tf.maximum(dec_mask, dec_causality_mask)\n",
    "\n",
    "    dec_enc_causality_mask = generate_causality_mask(tgt.shape[1], src.shape[1])\n",
    "    dec_enc_mask = tf.maximum(enc_mask, dec_enc_causality_mask)\n",
    "\n",
    "    return enc_mask, dec_enc_mask, dec_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "searching-terminal",
   "metadata": {},
   "source": [
    "#### Multi-head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "unauthorized-politics",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.depth = d_model // self.num_heads\n",
    "        \n",
    "        self.W_q = tf.keras.layers.Dense(d_model)\n",
    "        self.W_k = tf.keras.layers.Dense(d_model)\n",
    "        self.W_v = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "        self.linear = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask):\n",
    "        d_k = tf.cast(K.shape[-1], tf.float32)\n",
    "        QK = tf.matmul(Q, K, transpose_b=True)\n",
    "\n",
    "        scaled_qk = QK / tf.math.sqrt(d_k)\n",
    "\n",
    "        if mask is not None: scaled_qk += (mask * -1e9)  \n",
    "\n",
    "        attentions = tf.nn.softmax(scaled_qk, axis=-1)\n",
    "        out = tf.matmul(attentions, V)\n",
    "\n",
    "        return out, attentions\n",
    "        \n",
    "\n",
    "    def split_heads(self, x):\n",
    "        bsz = x.shape[0]\n",
    "        split_x = tf.reshape(x, (bsz, -1, self.num_heads, self.depth))\n",
    "        split_x = tf.transpose(split_x, perm=[0, 2, 1, 3])\n",
    "\n",
    "        return split_x\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        bsz = x.shape[0]\n",
    "        combined_x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        combined_x = tf.reshape(combined_x, (bsz, -1, self.d_model))\n",
    "\n",
    "        return combined_x\n",
    "\n",
    "    \n",
    "    def call(self, Q, K, V, mask):\n",
    "        WQ = self.W_q(Q)\n",
    "        WK = self.W_k(K)\n",
    "        WV = self.W_v(V)\n",
    "        \n",
    "        WQ_splits = self.split_heads(WQ)\n",
    "        WK_splits = self.split_heads(WK)\n",
    "        WV_splits = self.split_heads(WV)\n",
    "        \n",
    "        out, attention_weights = self.scaled_dot_product_attention(\n",
    "            WQ_splits, WK_splits, WV_splits, mask)\n",
    "                        \n",
    "        out = self.combine_heads(out)\n",
    "        out = self.linear(out)\n",
    "            \n",
    "        return out, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleasant-acquisition",
   "metadata": {},
   "source": [
    "#### Postion-wise Feed Forward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "pursuant-tractor",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "\n",
    "        self.fc1 = tf.keras.layers.Dense(d_ff, activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.fc2(out)\n",
    "            \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collectible-government",
   "metadata": {},
   "source": [
    "### 3.1.2 인코더 디코더 모델로 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "structural-conversation",
   "metadata": {},
   "source": [
    "#### Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cardiac-twelve",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "\n",
    "        \"\"\"\n",
    "        Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, enc_attn = self.enc_self_attn(out, out, out, mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "        \n",
    "        \"\"\"\n",
    "        Position-Wise Feed Forward Network\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "        \n",
    "        return out, enc_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moving-ontario",
   "metadata": {},
   "source": [
    "#### Decoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "meaningful-center",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.dec_self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "    \n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "\n",
    "        \"\"\"\n",
    "        Masked Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, dec_attn = self.dec_self_attn(out, out, out, padding_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        \"\"\"\n",
    "        Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out, dec_enc_attn = self.dec_self_attn(out, enc_out, enc_out, causality_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "        \n",
    "        \"\"\"\n",
    "        Position-Wise Feed Forward Network\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_3(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        return out, dec_attn, dec_enc_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interim-question",
   "metadata": {},
   "source": [
    "### 3.1.3 Transformer 모델 구축"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominican-gospel",
   "metadata": {},
   "source": [
    "#### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "nervous-consciousness",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.enc_layers = [EncoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                        for _ in range(n_layers)]\n",
    "    \n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "        out = x\n",
    "    \n",
    "        enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, enc_attn = self.enc_layers[i](out, mask)\n",
    "            enc_attns.append(enc_attn)\n",
    "        \n",
    "        return out, enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "historic-delivery",
   "metadata": {},
   "source": [
    "#### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eligible-airline",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.dec_layers = [DecoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                            for _ in range(n_layers)]\n",
    "                            \n",
    "                            \n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "        out = x\n",
    "    \n",
    "        dec_attns = list()\n",
    "        dec_enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, dec_attn, dec_enc_attn = \\\n",
    "            self.dec_layers[i](out, enc_out, causality_mask, padding_mask)\n",
    "\n",
    "            dec_attns.append(dec_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "\n",
    "        return out, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genetic-papua",
   "metadata": {},
   "source": [
    "#### Transformer 전체 모델 조립"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "italian-grove",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    src_vocab_size,\n",
    "                    tgt_vocab_size,\n",
    "                    pos_len,\n",
    "                    dropout=0.2,\n",
    "                    shared_fc=True,\n",
    "                    shared_emb=False):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "\n",
    "        if shared_emb:\n",
    "            self.enc_emb = self.dec_emb = \\\n",
    "            tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "        else:\n",
    "            self.enc_emb = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "            self.dec_emb = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n",
    "\n",
    "        self.pos_encoding = positional_encoding(pos_len, d_model)\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "\n",
    "        self.fc = tf.keras.layers.Dense(tgt_vocab_size)\n",
    "\n",
    "        self.shared_fc = shared_fc\n",
    "\n",
    "        if shared_fc:\n",
    "            self.fc.set_weights(tf.transpose(self.dec_emb.weights))\n",
    "\n",
    "    def embedding(self, emb, x):\n",
    "        seq_len = x.shape[1]\n",
    "\n",
    "        out = emb(x)\n",
    "\n",
    "        if self.shared_fc: out *= tf.math.sqrt(self.d_model)\n",
    "\n",
    "        out += self.pos_encoding[np.newaxis, ...][:, :seq_len, :]\n",
    "        out = self.do(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "        \n",
    "    def call(self, enc_in, dec_in, enc_mask, causality_mask, dec_mask):\n",
    "        enc_in = self.embedding(self.enc_emb, enc_in)\n",
    "        dec_in = self.embedding(self.dec_emb, dec_in)\n",
    "\n",
    "        enc_out, enc_attns = self.encoder(enc_in, enc_mask)\n",
    "        \n",
    "        dec_out, dec_attns, dec_enc_attns = \\\n",
    "        self.decoder(dec_in, enc_out, causality_mask, dec_mask)\n",
    "        \n",
    "        logits = self.fc(dec_out)\n",
    "        \n",
    "        return logits, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "numerical-offense",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    n_layers=2,\n",
    "    d_model=512,\n",
    "    n_heads=8,\n",
    "    d_ff=2048,\n",
    "    src_vocab_size=vocab_size,\n",
    "    tgt_vocab_size=vocab_size,\n",
    "    pos_len=200,\n",
    "    dropout=0.3,\n",
    "    shared_fc=True,\n",
    "    shared_emb=True)\n",
    "\n",
    "d_model = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grave-processor",
   "metadata": {},
   "source": [
    "#### Learning Rate Scheduler & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "configured-genetics",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = step ** -0.5\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        \n",
    "        return (self.d_model ** -0.5) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "psychological-replication",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = LearningRateScheduler(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate,\n",
    "                                        beta_1=0.9,\n",
    "                                        beta_2=0.98, \n",
    "                                        epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "induced-fantasy",
   "metadata": {},
   "source": [
    "#### Loss Function 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "rolled-malpractice",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "planned-intellectual",
   "metadata": {},
   "source": [
    "## 3.2 챗봇 모델 훈련"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reduced-guitar",
   "metadata": {},
   "source": [
    "#### 모델에 들어가는 훈련셋과 검증셋을 분리시킵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "arctic-notification",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인코더 입력 데이터 형태 : \n",
      "[[4020  192 7038   91    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0]]\n",
      "디코더 입력 데이터 형태 : \n",
      "[[  2 278   9 174   9  36   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   3]]\n"
     ]
    }
   ],
   "source": [
    "enc_input = tokenizer.tokenization(data[\"pre_Q\"], word_index, max_length)\n",
    "dec_input = tokenizer.tokenization(data[\"pre_A\"], word_index, max_length)\n",
    "dec_input = [[2] + list(i) + [3] for i in dec_input]\n",
    "dec_input = np.array(dec_input)\n",
    "\n",
    "print(f\"인코더 입력 데이터 형태 : \\n{enc_input[:1]}\")\n",
    "print(f\"디코더 입력 데이터 형태 : \\n{dec_input[:1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "driving-twins",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인코더-디코더 훈련 데이터 개수 : 37770\n",
      "인코더-디코더 검증 데이터 개수 : 3958\n",
      "\n",
      "\n",
      "인코더 훈련 데이터 길이 : 39\n",
      "디코더 훈련 데이터 길이 : 41\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(200)\n",
    "num_data = len(enc_input)\n",
    "\n",
    "range_idx = np.arange(0, num_data)\n",
    "random_idx = np.random.choice(range_idx, int(num_data * 0.1))\n",
    "val_idx = np.isin(range_idx, random_idx, invert = False)\n",
    "train_idx = np.isin(range_idx, random_idx, invert = True)\n",
    "\n",
    "enc_train = enc_input[train_idx]\n",
    "enc_val = enc_input[val_idx]\n",
    "dec_train = dec_input[train_idx]\n",
    "dec_val = dec_input[val_idx]\n",
    "\n",
    "print(f\"인코더-디코더 훈련 데이터 개수 : {enc_train.shape[0]}\")\n",
    "print(f\"인코더-디코더 검증 데이터 개수 : {enc_val.shape[0]}\")\n",
    "print(\"\\n\")\n",
    "print(f\"인코더 훈련 데이터 길이 : {enc_train.shape[1]}\")\n",
    "print(f\"디코더 훈련 데이터 길이 : {dec_train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "golden-deviation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acoustic-roberts",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "decreased-journalist",
   "metadata": {},
   "source": [
    "#### 모델 학습 train step 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "structured-rescue",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def train_step(src, tgt, model, optimizer):\n",
    "    tgt_in = tgt[:, :-1]  # Decoder의 input\n",
    "    gold = tgt[:, 1:]     # Decoder의 output과 비교하기 위해 right shift를 통해 생성한 최종 타겟\n",
    "\n",
    "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt_in)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        model(src, tgt_in, enc_mask, dec_enc_mask, dec_mask)\n",
    "        loss = loss_function(gold, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    return loss, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "happy-jefferson",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:9: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8b78ed6233e43938e2c96e45dd36195",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/591 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdbafcf903ef434b9570043e732946e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/591 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "315bc4e615db40bab6c1f60d956d34a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/591 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ada17c2fb5344ba800d9f95d2e56b63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/591 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "629ec8d833694169a091324179adeeb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/591 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4694f1ac45734e93936ccf3f6cc38ad2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/591 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d474c7f240a94e9887f9711083805a70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/591 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2f29804414b46a9a6d3ace2d9df2ab1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/591 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49674b0507974c2c98f35661bf7657c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/591 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27f35cf6316349cda2a30cff9a4056ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/591 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    \n",
    "    idx_list = list(range(0, enc_train.shape[0], BATCH_SIZE))\n",
    "    random.shuffle(idx_list)\n",
    "    t = tqdm_notebook(idx_list)\n",
    "\n",
    "    for (batch, idx) in enumerate(t):\n",
    "        batch_loss, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        train_step(enc_train[idx:idx+BATCH_SIZE],\n",
    "                    dec_train[idx:idx+BATCH_SIZE],\n",
    "                    transformer,\n",
    "                    optimizer)\n",
    "\n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        t.set_description_str('Epoch %2d' % (epoch + 1))\n",
    "        t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "criminal-portugal",
   "metadata": {},
   "source": [
    "## 3.3 챗봇 모델 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "female-feature",
   "metadata": {},
   "source": [
    "### 3.3.1 SmoothinFunction이 적용된 BLEU score 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "radio-basis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원문: ['많', '은', '자연어', '처리', '연구자', '들', '이', '트랜스포머', '를', '선호', '한다']\n",
      "번역문: ['적', '은', '자연어', '학', '개발자', '들', '가', '트랜스포머', '을', '선호', '한다', '요']\n",
      "BLEU Score: 8.190757052088229e-155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import SmoothingFunction, sentence_bleu\n",
    "\n",
    "#!pip install nltk # nltk가 설치되어 있지 않은 경우 주석 해제\n",
    "reference = \"많 은 자연어 처리 연구자 들 이 트랜스포머 를 선호 한다\".split()\n",
    "candidate = \"적 은 자연어 학 개발자 들 가 트랜스포머 을 선호 한다 요\".split()\n",
    "\n",
    "print(\"원문:\", reference)\n",
    "print(\"번역문:\", candidate)\n",
    "print(\"BLEU Score:\", sentence_bleu([reference], candidate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cooked-concrete",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.5\n",
      "BLEU-2: 0.18181818181818182\n",
      "BLEU-3: 0.010000000000000004\n",
      "BLEU-4: 0.011111111111111112\n",
      "\n",
      "BLEU-Total: 0.05637560315259291\n"
     ]
    }
   ],
   "source": [
    "def calculate_bleu(reference, candidate, weights=[0.25, 0.25, 0.25, 0.25]):\n",
    "    return sentence_bleu([reference],\n",
    "                         candidate,\n",
    "                         weights=weights,\n",
    "                         smoothing_function=SmoothingFunction().method1)  # smoothing_function 적용\n",
    "\n",
    "print(\"BLEU-1:\", calculate_bleu(reference, candidate, weights=[1, 0, 0, 0]))\n",
    "print(\"BLEU-2:\", calculate_bleu(reference, candidate, weights=[0, 1, 0, 0]))\n",
    "print(\"BLEU-3:\", calculate_bleu(reference, candidate, weights=[0, 0, 1, 0]))\n",
    "print(\"BLEU-4:\", calculate_bleu(reference, candidate, weights=[0, 0, 0, 1]))\n",
    "\n",
    "print(\"\\nBLEU-Total:\", calculate_bleu(reference, candidate))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tired-radio",
   "metadata": {},
   "source": [
    "### 3.3.2 모델 평가 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "juvenile-mainstream",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def evaluate(sentence, model, mecab, word_index, index_word):\n",
    "    if type(sentence) == str:        \n",
    "        sentence = utils.text_prep(sentence)\n",
    "\n",
    "        pieces = mecab.morphs(sentence)\n",
    "        tokens = [word_index[piece] for piece in pieces]\n",
    "        _input = tf.keras.preprocessing.sequence.pad_sequences([tokens],\n",
    "                                                               maxlen=enc_train.shape[-1],\n",
    "                                                               padding='post')\n",
    "    else:\n",
    "        pieces = [index_word[value] for value in sentence] # id 리스트로된 입력문장을 단어 리스트로 변환\n",
    "        _input = np.expand_dims(sentence, axis = 0) # str인 경우와 형태를 맞추기 위해서 차원을 늘림\n",
    "    ids = []\n",
    "    output = tf.expand_dims([word_index[\"<sos>\"]], 0)\n",
    "    \n",
    "    bleu_list = []\n",
    "    \n",
    "    for i in range(dec_train.shape[-1]):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = \\\n",
    "        generate_masks(_input, output)\n",
    "\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns =\\\n",
    "        transformer(_input, \n",
    "                    output,\n",
    "                    enc_padding_mask,\n",
    "                    combined_mask,\n",
    "                    dec_padding_mask)\n",
    "\n",
    "        predicted_id = \\\n",
    "        tf.argmax(tf.math.softmax(predictions, axis=-1)[0, -1]).numpy().item()  # predictions에 소프트맥스 함수를 적용하여 가장 큰 값의 인덱스를 predicted_id로 저장합니다.\n",
    "        \n",
    "        \n",
    "        \n",
    "        if word_index[\"<eos>\"] == predicted_id:\n",
    "            result = [index_word[value] for value in ids]    # 숫자를 문자열로 복원합니다.  \n",
    "            return pieces, result, enc_attns, dec_attns, dec_enc_attns\n",
    "        \n",
    "\n",
    "        ids.append(predicted_id)\n",
    "        \n",
    "        tmp = output.numpy()[0]\n",
    "        tmp = [index_word[i] for i in tmp]\n",
    "        print(pieces, tmp)\n",
    "        bleu_list += [calculate_bleu(pieces, tmp)]\n",
    "        output = tf.concat([output, tf.expand_dims([predicted_id], 0)], axis=-1)\n",
    "        \n",
    "        \n",
    "        \n",
    "    result = [index_word[value] for value in ids]  # 단어 리스트로 예측 결과물 출력\n",
    "    \n",
    "    \n",
    "    return pieces, result, enc_attns, dec_attns, dec_enc_attns, bleu_list\n",
    "\n",
    "def translate(sentence, model, mecab, word_index, index_word):\n",
    "    pieces, result, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "    evaluate(sentence, model, mecab, word_index, index_word)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "responsible-spanish",
   "metadata": {},
   "source": [
    "### 3.3.3 모델 BLEU 점수로 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "simplified-surgery",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sd', '카드', '망가졌', '어', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'] ['<sos>']\n",
      "['sd', '카드', '망가졌', '어', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'] ['<sos>', '다시']\n",
      "['sd', '카드', '망가졌', '어', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'] ['<sos>', '다시', '새로']\n",
      "['sd', '카드', '망가졌', '어', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'] ['<sos>', '다시', '새로', '사']\n",
      "['sd', '카드', '망가졌', '어', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'] ['<sos>', '다시', '새로', '사', '는']\n",
      "['sd', '카드', '망가졌', '어', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'] ['<sos>', '다시', '새로', '사', '는', '게']\n",
      "['sd', '카드', '망가졌', '어', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'] ['<sos>', '다시', '새로', '사', '는', '게', '마음']\n",
      "['sd', '카드', '망가졌', '어', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'] ['<sos>', '다시', '새로', '사', '는', '게', '마음', '편해요']\n",
      "['sd', '카드', '망가졌', '어', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'] ['<sos>', '다시', '새로', '사', '는', '게', '마음', '편해요', '지']\n",
      "['sd', '카드', '망가졌', '어', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'] ['<sos>', '다시', '새로', '사', '는', '게', '마음', '편해요', '지', '마세요']\n",
      "['sd', '카드', '망가졌', '어', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'] ['<sos>', '다시', '새로', '사', '는', '게', '마음', '편해요', '지', '마세요', '마음']\n",
      "['sd', '카드', '망가졌', '어', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'] ['<sos>', '다시', '새로', '사', '는', '게', '마음', '편해요', '지', '마세요', '마음', '이']\n",
      "['sd', '카드', '망가졌', '어', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'] ['<sos>', '다시', '새로', '사', '는', '게', '마음', '편해요', '지', '마세요', '마음', '이', '다시']\n",
      "['sd', '카드', '망가졌', '어', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'] ['<sos>', '다시', '새로', '사', '는', '게', '마음', '편해요', '지', '마세요', '마음', '이', '다시', '새로']\n",
      "['sd', '카드', '망가졌', '어', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'] ['<sos>', '다시', '새로', '사', '는', '게', '마음', '편해요', '지', '마세요', '마음', '이', '다시', '새로', '새로']\n",
      "['sd', '카드', '망가졌', '어', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'] ['<sos>', '다시', '새로', '사', '는', '게', '마음', '편해요', '지', '마세요', '마음', '이', '다시', '새로', '새로', '해']\n",
      "['sd', '카드', '망가졌', '어', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'] ['<sos>', '다시', '새로', '사', '는', '게', '마음', '편해요', '지', '마세요', '마음', '이', '다시', '새로', '새로', '해', '보']\n",
      "['sd', '카드', '망가졌', '어', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'] ['<sos>', '다시', '새로', '사', '는', '게', '마음', '편해요', '지', '마세요', '마음', '이', '다시', '새로', '새로', '해', '보', '세요']\n",
      "['sd', '카드', '망가졌', '어', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'] ['<sos>', '다시', '새로', '사', '는', '게', '마음', '편해요', '지', '마세요', '마음', '이', '다시', '새로', '새로', '해', '보', '세요', '마음']\n",
      "['sd', '카드', '망가졌', '어', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'] ['<sos>', '다시', '새로', '사', '는', '게', '마음', '편해요', '지', '마세요', '마음', '이', '다시', '새로', '새로', '해', '보', '세요', '마음', '새로']\n",
      "['sd', '카드', '망가졌', '어', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'] ['<sos>', '다시', '새로', '사', '는', '게', '마음', '편해요', '지', '마세요', '마음', '이', '다시', '새로', '새로', '해', '보', '세요', '마음', '새로', '성']\n",
      "['sd', '카드', '망가졌', '어', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'] ['<sos>', '다시', '새로', '사', '는', '게', '마음', '편해요', '지', '마세요', '마음', '이', '다시', '새로', '새로', '해', '보', '세요', '마음', '새로', '성', '이']\n",
      "['sd', '카드', '망가졌', '어', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'] ['<sos>', '다시', '새로', '사', '는', '게', '마음', '편해요', '지', '마세요', '마음', '이', '다시', '새로', '새로', '해', '보', '세요', '마음', '새로', '성', '이', '새로']\n",
      "['sd', '카드', '망가졌', '어', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'] ['<sos>', '다시', '새로', '사', '는', '게', '마음', '편해요', '지', '마세요', '마음', '이', '다시', '새로', '새로', '해', '보', '세요', '마음', '새로', '성', '이', '새로', '사']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sd', '카드', '망가졌', '어', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'] ['<sos>', '다시', '새로', '사', '는', '게', '마음', '편해요', '지', '마세요', '마음', '이', '다시', '새로', '새로', '해', '보', '세요', '마음', '새로', '성', '이', '새로', '사', '는']\n",
      "['sd', '카드', '망가졌', '어', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'] ['<sos>', '다시', '새로', '사', '는', '게', '마음', '편해요', '지', '마세요', '마음', '이', '다시', '새로', '새로', '해', '보', '세요', '마음', '새로', '성', '이', '새로', '사', '는', '거']\n",
      "['sd', '카드', '망가졌', '어', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'] ['<sos>', '다시', '새로', '사', '는', '게', '마음', '편해요', '지', '마세요', '마음', '이', '다시', '새로', '새로', '해', '보', '세요', '마음', '새로', '성', '이', '새로', '사', '는', '거', '예요']\n",
      "['sd', '카드', '망가졌', '어', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'] ['<sos>', '다시', '새로', '사', '는', '게', '마음', '편해요', '지', '마세요', '마음', '이', '다시', '새로', '새로', '해', '보', '세요', '마음', '새로', '성', '이', '새로', '사', '는', '거', '예요', '그러다가']\n",
      "['sd', '카드', '망가졌', '어', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'] ['<sos>', '다시', '새로', '사', '는', '게', '마음', '편해요', '지', '마세요', '마음', '이', '다시', '새로', '새로', '해', '보', '세요', '마음', '새로', '성', '이', '새로', '사', '는', '거', '예요', '그러다가', '마음']\n",
      "['sd', '카드', '망가졌', '어', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'] ['<sos>', '다시', '새로', '사', '는', '게', '마음', '편해요', '지', '마세요', '마음', '이', '다시', '새로', '새로', '해', '보', '세요', '마음', '새로', '성', '이', '새로', '사', '는', '거', '예요', '그러다가', '마음', '으로']\n",
      "['sd', '카드', '망가졌', '어', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'] ['<sos>', '다시', '새로', '사', '는', '게', '마음', '편해요', '지', '마세요', '마음', '이', '다시', '새로', '새로', '해', '보', '세요', '마음', '새로', '성', '이', '새로', '사', '는', '거', '예요', '그러다가', '마음', '으로', '새로']\n",
      "['sd', '카드', '망가졌', '어', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'] ['<sos>', '다시', '새로', '사', '는', '게', '마음', '편해요', '지', '마세요', '마음', '이', '다시', '새로', '새로', '해', '보', '세요', '마음', '새로', '성', '이', '새로', '사', '는', '거', '예요', '그러다가', '마음', '으로', '새로', '사']\n",
      "['sd', '카드', '망가졌', '어', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'] ['<sos>', '다시', '새로', '사', '는', '게', '마음', '편해요', '지', '마세요', '마음', '이', '다시', '새로', '새로', '해', '보', '세요', '마음', '새로', '성', '이', '새로', '사', '는', '거', '예요', '그러다가', '마음', '으로', '새로', '사', '는']\n",
      "['sd', '카드', '망가졌', '어', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'] ['<sos>', '다시', '새로', '사', '는', '게', '마음', '편해요', '지', '마세요', '마음', '이', '다시', '새로', '새로', '해', '보', '세요', '마음', '새로', '성', '이', '새로', '사', '는', '거', '예요', '그러다가', '마음', '으로', '새로', '사', '는', '거']\n",
      "['sd', '카드', '망가졌', '어', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'] ['<sos>', '다시', '새로', '사', '는', '게', '마음', '편해요', '지', '마세요', '마음', '이', '다시', '새로', '새로', '해', '보', '세요', '마음', '새로', '성', '이', '새로', '사', '는', '거', '예요', '그러다가', '마음', '으로', '새로', '사', '는', '거', '예요']\n",
      "['sd', '카드', '망가졌', '어', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'] ['<sos>', '다시', '새로', '사', '는', '게', '마음', '편해요', '지', '마세요', '마음', '이', '다시', '새로', '새로', '해', '보', '세요', '마음', '새로', '성', '이', '새로', '사', '는', '거', '예요', '그러다가', '마음', '으로', '새로', '사', '는', '거', '예요', '상대']\n",
      "['sd', '카드', '망가졌', '어', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'] ['<sos>', '다시', '새로', '사', '는', '게', '마음', '편해요', '지', '마세요', '마음', '이', '다시', '새로', '새로', '해', '보', '세요', '마음', '새로', '성', '이', '새로', '사', '는', '거', '예요', '그러다가', '마음', '으로', '새로', '사', '는', '거', '예요', '상대', '를']\n",
      "['sd', '카드', '망가졌', '어', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'] ['<sos>', '다시', '새로', '사', '는', '게', '마음', '편해요', '지', '마세요', '마음', '이', '다시', '새로', '새로', '해', '보', '세요', '마음', '새로', '성', '이', '새로', '사', '는', '거', '예요', '그러다가', '마음', '으로', '새로', '사', '는', '거', '예요', '상대', '를', '찾아보']\n",
      "['sd', '카드', '망가졌', '어', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'] ['<sos>', '다시', '새로', '사', '는', '게', '마음', '편해요', '지', '마세요', '마음', '이', '다시', '새로', '새로', '해', '보', '세요', '마음', '새로', '성', '이', '새로', '사', '는', '거', '예요', '그러다가', '마음', '으로', '새로', '사', '는', '거', '예요', '상대', '를', '찾아보', '세요']\n",
      "['sd', '카드', '망가졌', '어', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'] ['<sos>', '다시', '새로', '사', '는', '게', '마음', '편해요', '지', '마세요', '마음', '이', '다시', '새로', '새로', '해', '보', '세요', '마음', '새로', '성', '이', '새로', '사', '는', '거', '예요', '그러다가', '마음', '으로', '새로', '사', '는', '거', '예요', '상대', '를', '찾아보', '세요', '마음']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sd', '카드', '망가졌', '어', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'] ['<sos>', '다시', '새로', '사', '는', '게', '마음', '편해요', '지', '마세요', '마음', '이', '다시', '새로', '새로', '해', '보', '세요', '마음', '새로', '성', '이', '새로', '사', '는', '거', '예요', '그러다가', '마음', '으로', '새로', '사', '는', '거', '예요', '상대', '를', '찾아보', '세요', '마음', '으로']\n"
     ]
    }
   ],
   "source": [
    "p, r, e, da, dea, bleu = evaluate(enc_val[1], transformer, mecab, word_index, index_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "unauthorized-disney",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "excess-artwork",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_bleu(src_corpus, tgt_corpus, verbose=True):\n",
    "    total_score = 0.0\n",
    "    sample_size = len(tgt_corpus)\n",
    "\n",
    "    for idx in tqdm_notebook(range(sample_size)):\n",
    "        src_tokens = src_corpus[idx]\n",
    "        tgt_tokens = tgt_corpus[idx]\n",
    "        \n",
    "        src_sentence = [index_word[token] for token in src_tokens if token != 0]\n",
    "        src_sentence = \" \".join(src_sentence)\n",
    "        \n",
    "        reference = [index_word[token] for token in tgt_tokens  if token != 0]\n",
    "        reference = \" \".join(reference)\n",
    "        \n",
    "        candidate = translate(src_tokens, transformer, mecab, word_index, index_word)\n",
    "        candidate = \" \".join(candidate)\n",
    "        \n",
    "        \n",
    "        score = sentence_bleu([reference], candidate,\n",
    "                              smoothing_function=SmoothingFunction().method1)\n",
    "        total_score += score\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Source Sentence: \", src_sentence)\n",
    "            print(\"Model Prediction: \", candidate)\n",
    "            print(\"Real: \", reference)\n",
    "            print(\"Score: %lf\\n\" % score)\n",
    "\n",
    "    print(\"Num of Sample:\", sample_size)\n",
    "    print(\"Total Score:\", total_score / sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "valid-monster",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a0fba8757c14cbbafc81b6b4aaa86eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Sentence:  3 박 4 일 정도 놀 러 가 고 싶 다\n",
      "Model Prediction:  여행 은 언제나 좋 죠 ! 해 주 세요 ! ! 다시 글 로 죽 어요 당신 은 세요 ! 있 어요 ? 요 ? 여행 요 ? 요 ? 요 ? 여행 에게 좋 죠 요 ? 여행 요 ?\n",
      "Real:  <sos> 여행 은 언제나 좋 죠 <eos>\n",
      "Score: 0.134602\n",
      "\n",
      "Source Sentence:  sd 카드 망가졌 어\n",
      "Model Prediction:  다시 새로 사 는 게 마음 편해요 지 마세요 마음 이 다시 새로 새로 해 보 세요 마음 새로 성 이 새로 사 는 거 예요 그러다가 마음 으로 새로 사 는 거 예요 상대 를 찾아보 세요 마음 으로 새로\n",
      "Real:  <sos> 다시 새로 사 는 게 마음 편해요 <eos>\n",
      "Score: 0.168649\n",
      "\n",
      "Source Sentence:  가스 불 켜 고 나갔 어\n",
      "Model Prediction:  빨리 집 에 돌아가 서 끄 고 나오 세요 나오 세요 기분 이 될지 도 몰라요 조금 씩 그런가 봐요 여유 를 생기 면 모든 걸 해 주 지 나오 세요 기다릴 기 위해서 들 겠 죠 ? 그런가 봐요 기분\n",
      "Real:  <sos> 빨리 집 에 돌아가 서 끄 고 나오 세요 <eos>\n",
      "Score: 0.201877\n",
      "\n",
      "Num of Sample: 3\n",
      "Total Score: 0.1683759537594777\n"
     ]
    }
   ],
   "source": [
    "eval_bleu(enc_val[:3], dec_val[:3], True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
