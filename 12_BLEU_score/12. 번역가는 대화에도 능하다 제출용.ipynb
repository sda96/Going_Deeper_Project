{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ultimate-bikini",
   "metadata": {},
   "source": [
    "# 1. 서론"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boolean-universal",
   "metadata": {},
   "source": [
    "해당 [링크](https://github.com/songys/Chatbot_data)의 챗봇 데이터를 활용하여 고객이 문장을 입력하면 적절한 반응을 하는 Transformer 모델 기반의 챗봇을 구현해보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "initial-satisfaction",
   "metadata": {},
   "source": [
    "해당 과제를 하면서 Mecab() 형태소 분석기가 제대로 import 되어지지 않는 현상이 발생하였고, Vocab()과 관련된 에러가 발생하여 새로 Mecab을 설치하는 과정을 다른 jupyer noebook 에 정리해 놓았습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stable-manchester",
   "metadata": {},
   "source": [
    "#### 기본적으로 사용되는 패키지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "powered-kennedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from packages import utils, tokenizer # 사용자 지정 패키지입니다.\n",
    "from konlpy.tag import Mecab\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm import tqdm\n",
    "import gensim\n",
    "from tqdm.notebook import tqdm as tqdm_notebook\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dental-district",
   "metadata": {},
   "source": [
    "#### matplotlib 한글 깨짐 해결 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "placed-steal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한글 지원 폰트\n",
    "sns.set(font='NanumGothic')\n",
    "\n",
    "# 마이너스 부호 \n",
    "\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "played-handling",
   "metadata": {},
   "source": [
    "# 2. 본론"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expensive-tobago",
   "metadata": {},
   "source": [
    "## 2.1 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "uniform-romantic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 개수 : 11823\n",
      "컬럼 개수 : 3\n",
      "컬럼명 : Index(['Q', 'A', 'label'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "dic = utils.load_data_all(\"./data/ChatbotData.csv\")\n",
    "file_name = list(dic.keys())[0]\n",
    "data = dic[file_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "representative-vessel",
   "metadata": {},
   "source": [
    "## 2.2 데이터 전처리\n",
    "- 특수문자 제거\n",
    "- 결측 데이터 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enabling-sleeve",
   "metadata": {},
   "source": [
    "중복 데이터까지 제거하게 되면 너무 많은 데이터의 소실이 생기기 때문에 중복 데이터 제거는 하지 않겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "divided-overhead",
   "metadata": {},
   "source": [
    "### 2.2.1 특수문자 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "changed-adapter",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"pre_Q\"] = data[\"Q\"].apply(lambda x: utils.text_prep(x))\n",
    "data[\"pre_A\"] = data[\"A\"].apply(lambda x: utils.text_prep(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "muslim-going",
   "metadata": {},
   "source": [
    "### 2.2.2 결측 데이터 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "binding-pierce",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = utils.remove_nan(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designing-matrix",
   "metadata": {},
   "source": [
    "### 2.2.3 데이터 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "architectural-favorite",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "      <th>pre_Q</th>\n",
       "      <th>pre_A</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "      <td>0</td>\n",
       "      <td>[12, 시, 땡, !]</td>\n",
       "      <td>[하루, 가, 또, 가, 네요]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 지망, 학교, 떨어졌, 어]</td>\n",
       "      <td>[위로, 해, 드립니다]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "      <td>[3, 박, 4, 일, 놀, 러, 가, 고, 싶, 다]</td>\n",
       "      <td>[여행, 은, 언제나, 좋, 죠]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "      <td>[3, 박, 4, 일, 정도, 놀, 러, 가, 고, 싶, 다]</td>\n",
       "      <td>[여행, 은, 언제나, 좋, 죠]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "      <td>0</td>\n",
       "      <td>[ppl, 심하, 네]</td>\n",
       "      <td>[눈살, 이, 찌푸려, 지, 죠]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Q            A  label                               pre_Q  \\\n",
       "0           12시 땡!   하루가 또 가네요.      0                       [12, 시, 땡, !]   \n",
       "1      1지망 학교 떨어졌어    위로해 드립니다.      0                 [1, 지망, 학교, 떨어졌, 어]   \n",
       "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0      [3, 박, 4, 일, 놀, 러, 가, 고, 싶, 다]   \n",
       "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0  [3, 박, 4, 일, 정도, 놀, 러, 가, 고, 싶, 다]   \n",
       "4          PPL 심하네   눈살이 찌푸려지죠.      0                        [ppl, 심하, 네]   \n",
       "\n",
       "                pre_A  \n",
       "0   [하루, 가, 또, 가, 네요]  \n",
       "1       [위로, 해, 드립니다]  \n",
       "2  [여행, 은, 언제나, 좋, 죠]  \n",
       "3  [여행, 은, 언제나, 좋, 죠]  \n",
       "4  [눈살, 이, 찌푸려, 지, 죠]  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mecab = Mecab()\n",
    "data[\"pre_Q\"] = data[\"pre_Q\"].apply(lambda x: mecab.morphs(x))\n",
    "data[\"pre_A\"] = data[\"pre_A\"].apply(lambda x: mecab.morphs(x))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "random-freeware",
   "metadata": {},
   "source": [
    "## 2.3 데이터 증강"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defensive-circle",
   "metadata": {},
   "source": [
    "데이터 증강 방법은 Lexical Substitution을 사용할 것이며 해당 방법은 입력 문장의 토큰 중에서 무작위 토큰을 골라서 가장 유사성이 높은 단어로 대체시키는 방법으로 출력 문장은 유지시켜야합니다.\n",
    "\n",
    "유사성이 높은 단어의 기준은 사전 학습된 임베딩 벡터에서 무작위로 선택된 토큰을 넣어서 가장 유사성 점수가 높은 단어를 선택하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "seven-microphone",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = gensim.models.Word2Vec.load(\"./data/ko.bin\")\n",
    "copy_Q, copy_A = data[\"pre_Q\"].copy(), data[\"pre_A\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "through-decrease",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augmentation(data, copy_Q, copy_A):\n",
    "    augment_list = []\n",
    "    for Q, A in tqdm(zip(copy_Q, copy_A)):\n",
    "        cp_Q = Q.copy()\n",
    "        random_word = np.random.choice(cp_Q, 1)[0]\n",
    "        idx = cp_Q.index(random_word)\n",
    "        try:\n",
    "            cp_Q[idx] = word2vec.wv.most_similar(random_word)[0][0]\n",
    "        except:\n",
    "            continue\n",
    "        augment_list.append([cp_Q, A])\n",
    "\n",
    "    aug_dataset = pd.DataFrame(augment_list, columns = [\"pre_Q\", \"pre_A\"])\n",
    "    data = pd.concat([data, aug_dataset])\n",
    "    print(f\"데이터의 개수 : {data.shape[0]}\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "international-casino",
   "metadata": {},
   "source": [
    "기존 데이터의 개수는 11,823개인데 데이터 증강을 통하여 4배로 늘려서 41,684개의 데이터를 활용하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "noticed-scholar",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11823it [00:27, 436.44it/s]\n",
      "63it [00:00, 623.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터의 개수 : 21783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11823it [00:20, 563.23it/s]\n",
      "59it [00:00, 588.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터의 개수 : 31793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11823it [00:20, 564.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터의 개수 : 41757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 데이터 개수 3배로 늘리기\n",
    "for _ in range(3):\n",
    "    data = data_augmentation(data, copy_Q, copy_A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corrected-defensive",
   "metadata": {},
   "source": [
    "#### 문장별 토큰화된 단어 개수 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "intensive-preserve",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "data[\"len_Q\"] = data[\"pre_Q\"].apply(lambda x: len(x))\n",
    "count_len_Q = dict(Counter(data[\"len_Q\"]))\n",
    "sort_len_Q = sorted(count_len_Q.items(), key = lambda x : x[0])\n",
    "sort_len_Q = np.array(sort_len_Q)\n",
    "\n",
    "data[\"len_A\"] = data[\"pre_A\"].apply(lambda x: len(x))\n",
    "count_len_A = dict(Counter(data[\"len_A\"]))\n",
    "sort_len_A = sorted(count_len_A.items(), key = lambda x : x[0])\n",
    "sort_len_A = np.array(sort_len_A)\n",
    "\n",
    "\n",
    "plt.bar(sort_len_A[:,0], sort_len_A[:,1], label = \"A\")\n",
    "plt.bar(sort_len_Q[:,0], sort_len_Q[:,1], label = \"Q\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"토큰의 개수 및 문장의 길이\")\n",
    "plt.ylabel(\"문장의 개수\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intended-touch",
   "metadata": {},
   "source": [
    "#### 문장의 길이 20 이상인 데이터 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "respective-customs",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "      <th>pre_Q</th>\n",
       "      <th>pre_A</th>\n",
       "      <th>len_Q</th>\n",
       "      <th>len_A</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>가장 확실한 건 뭘까?</td>\n",
       "      <td>가장 확실한 시간은 오늘이에요. 어제와 내일을 놓고 고민하느라 시간을 낭비하지 마세요.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[가장, 확실, 한, 건, 뭘까, ?]</td>\n",
       "      <td>[가장, 확실, 한, 시간, 은, 오늘, 이, 에요, 어제, 와, 내일, 을, 놓,...</td>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>가족 있어?</td>\n",
       "      <td>저를 만들어 준 사람을 부모님, 저랑 이야기해 주는 사람을 친구로 생각하고 있어요</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[가족, 있, 어, ?]</td>\n",
       "      <td>[저, 를, 만들, 어, 준, 사람, 을, 부모, 님, ,, 저, 랑, 이야기, 해...</td>\n",
       "      <td>4</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>가족관계 알려 줘</td>\n",
       "      <td>저를 만들어 준 사람을 부모님, 저랑 이야기해 주는 사람을 친구로 생각하고 있어요</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[가족, 관계, 알려, 줘]</td>\n",
       "      <td>[저, 를, 만들, 어, 준, 사람, 을, 부모, 님, ,, 저, 랑, 이야기, 해...</td>\n",
       "      <td>4</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>가족이 누구야?</td>\n",
       "      <td>저를 만들어 준 사람을 부모님, 저랑 이야기해 주는 사람을 친구로 생각하고 있어요</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[가족, 이, 누구, 야, ?]</td>\n",
       "      <td>[저, 를, 만들, 어, 준, 사람, 을, 부모, 님, ,, 저, 랑, 이야기, 해...</td>\n",
       "      <td>5</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588</th>\n",
       "      <td>나는 왜 이렇게 태어났을까?</td>\n",
       "      <td>서로 다르게 태어난 이유는 저마다의 목소리를 내기 위해서예요. 자신의 목소리를 들어...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[나, 는, 왜, 이렇게, 태어났, 을까, ?]</td>\n",
       "      <td>[서로, 다르, 게, 태어난, 이유, 는, 저마다, 의, 목소리, 를, 내, 기, ...</td>\n",
       "      <td>7</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9912</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[하루, 종일, 같이, 있, 고, 싶, 어서]</td>\n",
       "      <td>[서로, 떨어져서, 그리워할, 시간, 을, 적당히, 만들, 어, 주, 는, 게, 관...</td>\n",
       "      <td>7</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9913</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[하루, 종일, 붙, 어, 있, 고, 겠, 어]</td>\n",
       "      <td>[뭐, 든, 함께, 하, 려는, 것, 도, 좋, 겠, 지만, 오래, 붙, 어, 잇,...</td>\n",
       "      <td>8</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9948</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[혼자, 싫어하, 는, 게, 이렇게, 힘든, 적, 은, 처음, 이, 에요]</td>\n",
       "      <td>[사랑, 은, 더, 잘, 하, 게, 되, 지, 않, 고, 다시, 영, 에서, 시작,...</td>\n",
       "      <td>11</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9955</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[확실히, 좋, 아서, 하, 는, 데, 도, 관심, 있, 는, 거, 티, 안, 내,...</td>\n",
       "      <td>[많이, 있, 어요]</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9959</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[회식, 중, 이, 라고, 하, 는데, 협조, 이, 안, 돼]</td>\n",
       "      <td>[정신, 없이, 바쁠, 지도, 몰라요, 조금, 만, 더, 기다려, 보, 고, 물, ...</td>\n",
       "      <td>10</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>391 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Q                                                  A  \\\n",
       "22       가장 확실한 건 뭘까?   가장 확실한 시간은 오늘이에요. 어제와 내일을 놓고 고민하느라 시간을 낭비하지 마세요.   \n",
       "26             가족 있어?      저를 만들어 준 사람을 부모님, 저랑 이야기해 주는 사람을 친구로 생각하고 있어요   \n",
       "27          가족관계 알려 줘      저를 만들어 준 사람을 부모님, 저랑 이야기해 주는 사람을 친구로 생각하고 있어요   \n",
       "35           가족이 누구야?      저를 만들어 준 사람을 부모님, 저랑 이야기해 주는 사람을 친구로 생각하고 있어요   \n",
       "588   나는 왜 이렇게 태어났을까?  서로 다르게 태어난 이유는 저마다의 목소리를 내기 위해서예요. 자신의 목소리를 들어...   \n",
       "...               ...                                                ...   \n",
       "9912              NaN                                                NaN   \n",
       "9913              NaN                                                NaN   \n",
       "9948              NaN                                                NaN   \n",
       "9955              NaN                                                NaN   \n",
       "9959              NaN                                                NaN   \n",
       "\n",
       "      label                                              pre_Q  \\\n",
       "22      0.0                              [가장, 확실, 한, 건, 뭘까, ?]   \n",
       "26      0.0                                      [가족, 있, 어, ?]   \n",
       "27      0.0                                    [가족, 관계, 알려, 줘]   \n",
       "35      0.0                                  [가족, 이, 누구, 야, ?]   \n",
       "588     0.0                         [나, 는, 왜, 이렇게, 태어났, 을까, ?]   \n",
       "...     ...                                                ...   \n",
       "9912    NaN                          [하루, 종일, 같이, 있, 고, 싶, 어서]   \n",
       "9913    NaN                         [하루, 종일, 붙, 어, 있, 고, 겠, 어]   \n",
       "9948    NaN          [혼자, 싫어하, 는, 게, 이렇게, 힘든, 적, 은, 처음, 이, 에요]   \n",
       "9955    NaN  [확실히, 좋, 아서, 하, 는, 데, 도, 관심, 있, 는, 거, 티, 안, 내,...   \n",
       "9959    NaN                 [회식, 중, 이, 라고, 하, 는데, 협조, 이, 안, 돼]   \n",
       "\n",
       "                                                  pre_A  len_Q  len_A  \n",
       "22    [가장, 확실, 한, 시간, 은, 오늘, 이, 에요, 어제, 와, 내일, 을, 놓,...      6     23  \n",
       "26    [저, 를, 만들, 어, 준, 사람, 을, 부모, 님, ,, 저, 랑, 이야기, 해...      4     25  \n",
       "27    [저, 를, 만들, 어, 준, 사람, 을, 부모, 님, ,, 저, 랑, 이야기, 해...      4     25  \n",
       "35    [저, 를, 만들, 어, 준, 사람, 을, 부모, 님, ,, 저, 랑, 이야기, 해...      5     25  \n",
       "588   [서로, 다르, 게, 태어난, 이유, 는, 저마다, 의, 목소리, 를, 내, 기, ...      7     22  \n",
       "...                                                 ...    ...    ...  \n",
       "9912  [서로, 떨어져서, 그리워할, 시간, 을, 적당히, 만들, 어, 주, 는, 게, 관...      7     24  \n",
       "9913  [뭐, 든, 함께, 하, 려는, 것, 도, 좋, 겠, 지만, 오래, 붙, 어, 잇,...      8     28  \n",
       "9948  [사랑, 은, 더, 잘, 하, 게, 되, 지, 않, 고, 다시, 영, 에서, 시작,...     11     21  \n",
       "9955                                        [많이, 있, 어요]     32      3  \n",
       "9959  [정신, 없이, 바쁠, 지도, 몰라요, 조금, 만, 더, 기다려, 보, 고, 물, ...     10     21  \n",
       "\n",
       "[391 rows x 7 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "more_length = 20\n",
    "data.loc[(data[\"len_Q\"] > more_length) | (data[\"len_A\"] > more_length), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enclosed-offering",
   "metadata": {},
   "source": [
    "#### 문장의 길이 2 미만인 데이터 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "colonial-efficiency",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "      <th>pre_Q</th>\n",
       "      <th>pre_A</th>\n",
       "      <th>len_Q</th>\n",
       "      <th>len_A</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>건방져</td>\n",
       "      <td>기분이 나쁘셨나봐요.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[건방져]</td>\n",
       "      <td>[기분, 이, 나쁘, 셨, 나, 봐요]</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>겁난다</td>\n",
       "      <td>용기 내보세요.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[겁난다]</td>\n",
       "      <td>[용기, 내보, 세요]</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>결혼하면 좋아?</td>\n",
       "      <td>해봐요.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[결혼, 하, 면, 좋, 아, ?]</td>\n",
       "      <td>[해봐요]</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>결혼해야 하나</td>\n",
       "      <td>해봐요.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[결혼, 해야, 하나]</td>\n",
       "      <td>[해봐요]</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>굿모닝</td>\n",
       "      <td>좋은 아침이에요.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[굿모닝]</td>\n",
       "      <td>[좋, 은, 아침, 이, 에요]</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7345</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[굉]</td>\n",
       "      <td>[마음, 이, 복잡, 한가요]</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7485</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[회개]</td>\n",
       "      <td>[성공, 하, 길, 바랄, 게요]</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7986</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[너, 판단, 이, 났, 어]</td>\n",
       "      <td>[어머나]</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8268</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[슬픔, 꾼]</td>\n",
       "      <td>[부러워요]</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9600</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[짝, 남의, 이, 내, 가, 좋, 아, 하, 는, 거, 알, 아, 버림]</td>\n",
       "      <td>[이런]</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>384 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Q            A  label                                      pre_Q  \\\n",
       "118        건방져  기분이 나쁘셨나봐요.    0.0                                      [건방져]   \n",
       "123        겁난다     용기 내보세요.    0.0                                      [겁난다]   \n",
       "157   결혼하면 좋아?         해봐요.    0.0                        [결혼, 하, 면, 좋, 아, ?]   \n",
       "165    결혼해야 하나         해봐요.    0.0                               [결혼, 해야, 하나]   \n",
       "293        굿모닝    좋은 아침이에요.    0.0                                      [굿모닝]   \n",
       "...        ...          ...    ...                                        ...   \n",
       "7345       NaN          NaN    NaN                                        [굉]   \n",
       "7485       NaN          NaN    NaN                                       [회개]   \n",
       "7986       NaN          NaN    NaN                           [너, 판단, 이, 났, 어]   \n",
       "8268       NaN          NaN    NaN                                    [슬픔, 꾼]   \n",
       "9600       NaN          NaN    NaN  [짝, 남의, 이, 내, 가, 좋, 아, 하, 는, 거, 알, 아, 버림]   \n",
       "\n",
       "                      pre_A  len_Q  len_A  \n",
       "118   [기분, 이, 나쁘, 셨, 나, 봐요]      1      6  \n",
       "123            [용기, 내보, 세요]      1      3  \n",
       "157                   [해봐요]      6      1  \n",
       "165                   [해봐요]      3      1  \n",
       "293       [좋, 은, 아침, 이, 에요]      1      5  \n",
       "...                     ...    ...    ...  \n",
       "7345       [마음, 이, 복잡, 한가요]      1      4  \n",
       "7485     [성공, 하, 길, 바랄, 게요]      1      5  \n",
       "7986                  [어머나]      5      1  \n",
       "8268                 [부러워요]      2      1  \n",
       "9600                   [이런]     13      1  \n",
       "\n",
       "[384 rows x 7 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "less_length = 2\n",
    "data.loc[(data[\"len_Q\"] < less_length) | (data[\"len_A\"] < less_length), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amateur-mumbai",
   "metadata": {},
   "source": [
    "토큰의 개수가 너무 적거나 너무 많다고 이상한 문장이 포함되어 있는 것은 아닌 것으로 판단되기 때문에 가능한 모든 데이터를 사용하도록 하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thrown-mercy",
   "metadata": {},
   "source": [
    "#### 토큰화 사전 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "loaded-generator",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어장 크기 : 8114\n"
     ]
    }
   ],
   "source": [
    "max_length = max(max(data[\"len_Q\"]), max(data[\"len_A\"]))\n",
    "raw_data = pd.concat([data[\"pre_Q\"], data[\"pre_A\"]])\n",
    "vocab_dictionary = tokenizer.bin_dict(raw_data)\n",
    "vocab_size = len(vocab_dictionary)\n",
    "print(f\"단어장 크기 : {vocab_size}\")\n",
    "word_index, index_word = tokenizer.word_index(vocab_dictionary, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "living-bubble",
   "metadata": {},
   "source": [
    "#### 인코더 디코더 모델의 입출력 형태 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "frozen-reason",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"decoder_input\"] = data[\"pre_A\"].apply(lambda x : [\"<sos>\"] + x)\n",
    "data[\"decoder_output\"] = data[\"pre_A\"].apply(lambda x : x + [\"<eos>\"])\n",
    "data[\"encoder_input\"] = data[\"pre_Q\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decimal-photograph",
   "metadata": {},
   "source": [
    "#### 단어들을 사전 id값으로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "essential-allergy",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((41757, 40), (41757, 40))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input = tokenizer.tokenization(data[\"encoder_input\"], word_index, max_length + 1)\n",
    "decoder_input = tokenizer.tokenization(data[\"decoder_input\"], word_index, max_length)\n",
    "decoder_output = tokenizer.tokenization(data[\"decoder_output\"], word_index, max_length)\n",
    "\n",
    "encoder_input.shape, decoder_input.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endangered-helping",
   "metadata": {},
   "source": [
    "#### 변환값과 실제 단어 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "demanding-nightmare",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 320 5048  548 1132   12    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "['1', '지망', '학교', '떨어졌', '어', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "1     [1, 지망, 학교, 떨어졌, 어]\n",
      "1    [1, 지망, 학교, 떨어졌, 어서]\n",
      "1    [1, 지망, 학교의, 떨어졌, 어]\n",
      "1     [1, 중퇴, 학교, 떨어졌, 어]\n",
      "Name: encoder_input, dtype: object\n",
      "\n",
      "\n",
      "[   2  550   15 1763    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "['<sos>', '위로', '해', '드립니다', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "1    [<sos>, 위로, 해, 드립니다]\n",
      "1    [<sos>, 위로, 해, 드립니다]\n",
      "1    [<sos>, 위로, 해, 드립니다]\n",
      "1    [<sos>, 위로, 해, 드립니다]\n",
      "Name: decoder_input, dtype: object\n",
      "\n",
      "\n",
      "[ 550   15 1763    3    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "['위로', '해', '드립니다', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "1    [위로, 해, 드립니다, <eos>]\n",
      "1    [위로, 해, 드립니다, <eos>]\n",
      "1    [위로, 해, 드립니다, <eos>]\n",
      "1    [위로, 해, 드립니다, <eos>]\n",
      "Name: decoder_output, dtype: object\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "print(encoder_input[i])\n",
    "print([index_word[k] for k in encoder_input[i]])\n",
    "print(data[\"encoder_input\"][i])\n",
    "print(\"\\n\")\n",
    "print(decoder_input[i])\n",
    "print([index_word[k] for k in decoder_input[i]])\n",
    "print(data[\"decoder_input\"][i])\n",
    "print(\"\\n\")\n",
    "print(decoder_output[i])\n",
    "print([index_word[k] for k in decoder_output[i]])\n",
    "print(data[\"decoder_output\"][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unusual-roommate",
   "metadata": {},
   "source": [
    "## 3.1 챗봇 모델 구축"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vanilla-increase",
   "metadata": {},
   "source": [
    "### 3.1.1 Transformer 세부 모듈 구축"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worst-snapshot",
   "metadata": {},
   "source": [
    "#### Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "productive-modem",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(pos, d_model):\n",
    "    def cal_angle(position, i):\n",
    "        return position / np.power(10000, int(i) / d_model)\n",
    "    \n",
    "    \n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, i) for i in range(d_model)]\n",
    "    \n",
    "    # (position, dimension) 생성\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(pos)])\n",
    "\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
    "\n",
    "    return sinusoid_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bigger-wiring",
   "metadata": {},
   "source": [
    "#### Masking Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "copyrighted-garden",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def generate_causality_mask(src_len, tgt_len):\n",
    "    mask = 1 - np.cumsum(np.eye(src_len, tgt_len), 0)\n",
    "    return tf.cast(mask, tf.float32)\n",
    "\n",
    "def generate_masks(src, tgt):\n",
    "    enc_mask = generate_padding_mask(src)\n",
    "    dec_mask = generate_padding_mask(tgt)\n",
    "\n",
    "    dec_causality_mask = generate_causality_mask(tgt.shape[1], tgt.shape[1])\n",
    "    dec_mask = tf.maximum(dec_mask, dec_causality_mask)\n",
    "\n",
    "    dec_enc_causality_mask = generate_causality_mask(tgt.shape[1], src.shape[1])\n",
    "    dec_enc_mask = tf.maximum(enc_mask, dec_enc_causality_mask)\n",
    "\n",
    "    return enc_mask, dec_enc_mask, dec_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separate-research",
   "metadata": {},
   "source": [
    "#### Multi-head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dominant-transportation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.depth = d_model // self.num_heads\n",
    "        \n",
    "        self.W_q = tf.keras.layers.Dense(d_model)\n",
    "        self.W_k = tf.keras.layers.Dense(d_model)\n",
    "        self.W_v = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "        self.linear = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask):\n",
    "        d_k = tf.cast(K.shape[-1], tf.float32)\n",
    "        QK = tf.matmul(Q, K, transpose_b=True)\n",
    "\n",
    "        scaled_qk = QK / tf.math.sqrt(d_k)\n",
    "\n",
    "        if mask is not None: scaled_qk += (mask * -1e9)  \n",
    "\n",
    "        attentions = tf.nn.softmax(scaled_qk, axis=-1)\n",
    "        out = tf.matmul(attentions, V)\n",
    "\n",
    "        return out, attentions\n",
    "        \n",
    "\n",
    "    def split_heads(self, x):\n",
    "        bsz = x.shape[0]\n",
    "        split_x = tf.reshape(x, (bsz, -1, self.num_heads, self.depth))\n",
    "        split_x = tf.transpose(split_x, perm=[0, 2, 1, 3])\n",
    "\n",
    "        return split_x\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        bsz = x.shape[0]\n",
    "        combined_x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        combined_x = tf.reshape(combined_x, (bsz, -1, self.d_model))\n",
    "\n",
    "        return combined_x\n",
    "\n",
    "    \n",
    "    def call(self, Q, K, V, mask):\n",
    "        WQ = self.W_q(Q)\n",
    "        WK = self.W_k(K)\n",
    "        WV = self.W_v(V)\n",
    "        \n",
    "        WQ_splits = self.split_heads(WQ)\n",
    "        WK_splits = self.split_heads(WK)\n",
    "        WV_splits = self.split_heads(WV)\n",
    "        \n",
    "        out, attention_weights = self.scaled_dot_product_attention(\n",
    "            WQ_splits, WK_splits, WV_splits, mask)\n",
    "                        \n",
    "        out = self.combine_heads(out)\n",
    "        out = self.linear(out)\n",
    "            \n",
    "        return out, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "renewable-division",
   "metadata": {},
   "source": [
    "#### Postion-wise Feed Forward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "popular-hacker",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "\n",
    "        self.fc1 = tf.keras.layers.Dense(d_ff, activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.fc2(out)\n",
    "            \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "following-moscow",
   "metadata": {},
   "source": [
    "### 3.1.2 인코더 디코더 모델로 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honest-criterion",
   "metadata": {},
   "source": [
    "#### Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "numeric-affiliation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "\n",
    "        \"\"\"\n",
    "        Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, enc_attn = self.enc_self_attn(out, out, out, mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "        \n",
    "        \"\"\"\n",
    "        Position-Wise Feed Forward Network\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "        \n",
    "        return out, enc_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protected-liability",
   "metadata": {},
   "source": [
    "#### Decoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "lucky-individual",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.dec_self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "    \n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "\n",
    "        \"\"\"\n",
    "        Masked Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, dec_attn = self.dec_self_attn(out, out, out, padding_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        \"\"\"\n",
    "        Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        # LMS 오타로 추정되는 부분 dec_self_attn으로 되있음\n",
    "        out, dec_enc_attn = self.enc_dec_attn(out, enc_out, enc_out, causality_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "        \n",
    "        \"\"\"\n",
    "        Position-Wise Feed Forward Network\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_3(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        return out, dec_attn, dec_enc_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quantitative-bridal",
   "metadata": {},
   "source": [
    "### 3.1.3 Transformer 모델 구축"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unnecessary-finland",
   "metadata": {},
   "source": [
    "#### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "naked-motivation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.enc_layers = [EncoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                        for _ in range(n_layers)]\n",
    "    \n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "        out = x\n",
    "    \n",
    "        enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, enc_attn = self.enc_layers[i](out, mask)\n",
    "            enc_attns.append(enc_attn)\n",
    "        \n",
    "        return out, enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excess-entertainment",
   "metadata": {},
   "source": [
    "#### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "chief-valuable",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.dec_layers = [DecoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                            for _ in range(n_layers)]\n",
    "                            \n",
    "                            \n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "        out = x\n",
    "    \n",
    "        dec_attns = list()\n",
    "        dec_enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, dec_attn, dec_enc_attn = \\\n",
    "            self.dec_layers[i](out, enc_out, causality_mask, padding_mask)\n",
    "\n",
    "            dec_attns.append(dec_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "\n",
    "        return out, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informed-enhancement",
   "metadata": {},
   "source": [
    "#### Transformer 전체 모델 조립"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "guided-paper",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    src_vocab_size,\n",
    "                    tgt_vocab_size,\n",
    "                    pos_len,\n",
    "                    dropout=0.2,\n",
    "                    shared_fc=True,\n",
    "                    shared_emb=False):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "\n",
    "        if shared_emb:\n",
    "            self.enc_emb = self.dec_emb = \\\n",
    "            tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "        else:\n",
    "            self.enc_emb = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "            self.dec_emb = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n",
    "\n",
    "        self.pos_encoding = positional_encoding(pos_len, d_model)\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "\n",
    "        self.fc = tf.keras.layers.Dense(tgt_vocab_size)\n",
    "\n",
    "        self.shared_fc = shared_fc\n",
    "\n",
    "        if shared_fc:\n",
    "            self.fc.set_weights(tf.transpose(self.dec_emb.weights))\n",
    "\n",
    "    def embedding(self, emb, x):\n",
    "        seq_len = x.shape[1]\n",
    "\n",
    "        out = emb(x)\n",
    "\n",
    "        if self.shared_fc: out *= tf.math.sqrt(self.d_model)\n",
    "\n",
    "        out += self.pos_encoding[np.newaxis, ...][:, :seq_len, :]\n",
    "        out = self.do(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "        \n",
    "    def call(self, enc_in, dec_in, enc_mask, causality_mask, dec_mask):\n",
    "        enc_in = self.embedding(self.enc_emb, enc_in)\n",
    "        dec_in = self.embedding(self.dec_emb, dec_in)\n",
    "\n",
    "        enc_out, enc_attns = self.encoder(enc_in, enc_mask)\n",
    "        \n",
    "        dec_out, dec_attns, dec_enc_attns = \\\n",
    "        self.decoder(dec_in, enc_out, causality_mask, dec_mask)\n",
    "        \n",
    "        logits = self.fc(dec_out)\n",
    "        \n",
    "        return logits, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "metric-annex",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    n_layers=2,\n",
    "    d_model=512,\n",
    "    n_heads=8,\n",
    "    d_ff=2048,\n",
    "    src_vocab_size=vocab_size,\n",
    "    tgt_vocab_size=vocab_size,\n",
    "    pos_len=200,\n",
    "    dropout=0.3,\n",
    "    shared_fc=True,\n",
    "    shared_emb=True)\n",
    "\n",
    "d_model = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handmade-hunger",
   "metadata": {},
   "source": [
    "#### Learning Rate Scheduler & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "magnetic-aberdeen",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = step ** -0.5\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        \n",
    "        return (self.d_model ** -0.5) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "technical-virtue",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = LearningRateScheduler(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate,\n",
    "                                        beta_1=0.9,\n",
    "                                        beta_2=0.98, \n",
    "                                        epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "republican-computer",
   "metadata": {},
   "source": [
    "#### Loss Function 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "empirical-illinois",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secure-village",
   "metadata": {},
   "source": [
    "## 3.2 챗봇 모델 훈련"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stupid-switch",
   "metadata": {},
   "source": [
    "#### 모델에 들어가는 훈련셋과 검증셋을 분리시킵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cleared-hotel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인코더 입력 데이터 형태 : \n",
      "[[3228  187 7011   92    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]]\n",
      "디코더 입력 데이터 형태 : \n",
      "[[  2 278   9 170   9  34   3   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "enc_input = tokenizer.tokenization(data[\"pre_Q\"], word_index, max_length + 1)\n",
    "dec_input = [[\"<sos>\"] + i + [\"<eos>\"] for i in data[\"pre_A\"]]\n",
    "dec_input = tokenizer.tokenization(dec_input, word_index, max_length - 1)\n",
    "dec_input = np.array(dec_input)\n",
    "\n",
    "print(f\"인코더 입력 데이터 형태 : \\n{enc_input[:1]}\")\n",
    "print(f\"디코더 입력 데이터 형태 : \\n{dec_input[:1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automatic-sally",
   "metadata": {},
   "source": [
    "기존 ---> [2, 15, 43, 34, 5, 0, 0, 0, 0, 0, 0, 0, 3]  \n",
    "앞서 LMS 노드에 나온 디코더부분의 입력 데이터의 형태는 맨 앞에 시작토큰(2), 맨 뒤에 종료토큰(3)이 존재하는 형태로 간단하게 앞 뒤로 인덱싱을 하면 되었습니다.\n",
    "\n",
    "하지만 해당 방식은 종료토큰을 제대로 학습하지 못하여 올바른 상황에서 종료토큰이 나오지 못하고 문장을 예측하면 제대로된 답을 하여도 쓸데 없이 문장이 길어지는 문제가 발생하였습니다.\n",
    "\n",
    "이 문제를 해결하기 위해서 종료토큰을 패딩토큰 앞에 미리 부여하여 종료토큰의 위치를 바꾼 결과 종료토큰을 잘 학습하는 효과가 있었습니다.  \n",
    "변경 ---> [2, 15, 43, 34, 5, 3, 0, 0, 0, 0, 0, 0, 0]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "adjustable-nursery",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(200)\n",
    "num_data = len(enc_input)\n",
    "\n",
    "range_idx = np.arange(0, num_data)\n",
    "random_idx = np.random.choice(range_idx, int(num_data * 0.1))\n",
    "val_idx = np.isin(range_idx, random_idx, invert = False)\n",
    "train_idx = np.isin(range_idx, random_idx, invert = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detected-advertising",
   "metadata": {},
   "source": [
    "훈련셋과 검증셋 데이터를 분리하기 위해서 훈련셋으로 사용할 무작위 인덱스를 뽑고 뽑히지 않은 나머지 인덱스는 검증셋 데이터로 사용합니다.\n",
    "훈련 인덱스와 검증 인덱스를 ```np.isin()``` 함수를 통하여 나누어 줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "theoretical-eagle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(train_idx == val_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flush-grove",
   "metadata": {},
   "source": [
    "나누어진 인덱스에서 서로 겹치는 값이 존재하면 적어도 1보다 큰 값이 나와야 하지만 서로 겹치는 값이 없는 경우 0을 출력할 수 있기 때문에 훈련셋과 검증셋이 서로 겹치는 인덱스는 없다고 볼 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "offshore-pastor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인코더-디코더 훈련 데이터 개수 : 37796\n",
      "인코더-디코더 검증 데이터 개수 : 3961\n",
      "\n",
      "\n",
      "인코더 훈련 데이터 길이 : 40\n",
      "디코더 훈련 데이터 길이 : 41\n"
     ]
    }
   ],
   "source": [
    "enc_train = enc_input[train_idx]\n",
    "enc_val = enc_input[val_idx]\n",
    "dec_train = dec_input[train_idx]\n",
    "dec_val = dec_input[val_idx]\n",
    "\n",
    "print(f\"인코더-디코더 훈련 데이터 개수 : {enc_train.shape[0]}\")\n",
    "print(f\"인코더-디코더 검증 데이터 개수 : {enc_val.shape[0]}\")\n",
    "print(\"\\n\")\n",
    "print(f\"인코더 훈련 데이터 길이 : {enc_train.shape[1]}\")\n",
    "print(f\"디코더 훈련 데이터 길이 : {dec_train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "introductory-natural",
   "metadata": {},
   "source": [
    "#### 모델 학습 train step 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "outside-jewel",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def train_step(src, tgt_in, gold, model, optimizer):\n",
    "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt_in)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        model(src, tgt_in, enc_mask, dec_enc_mask, dec_mask)\n",
    "        loss = loss_function(gold, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    return loss, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "limiting-python",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8443d676de8346ef8c4295e48ab869fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/296 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0ca55cef2564d61af32b58cf80ecc9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/296 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e805f9f9542342eca3d5fdfc86e9661d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/296 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c2e9d0fd2a84f3394fa18cb328ba052",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/296 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "863efb083d6d470eaac6c6c5fd530c16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/296 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "EPOCHS = 5\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    \n",
    "    idx_list = list(range(0, enc_train.shape[0], BATCH_SIZE))\n",
    "    random.shuffle(idx_list)\n",
    "    t = tqdm_notebook(idx_list)\n",
    "\n",
    "    for (batch, idx) in enumerate(t):\n",
    "        \n",
    "        dec_train_batch = dec_train[idx:idx+BATCH_SIZE]\n",
    "        length = dec_train_batch.shape[-1]\n",
    "        # decoder_input이므로 <eos> 토큰을 제거합니다.\n",
    "        tgt_in = dec_train_batch[dec_train_batch != 3].reshape(-1, (length - 1))\n",
    "        # decoder_output이므로 <sos> 토큰을 제거합니다.\n",
    "        gold = dec_train_batch[dec_train_batch != 2].reshape(-1, (length - 1))\n",
    "        \n",
    "        batch_loss, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        train_step(enc_train[idx:idx+BATCH_SIZE],\n",
    "                   tgt_in,\n",
    "                   gold,\n",
    "                   transformer,\n",
    "                   optimizer)\n",
    "\n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        t.set_description_str('Epoch %2d' % (epoch + 1))\n",
    "        t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civilian-carnival",
   "metadata": {},
   "source": [
    "## 3.3 챗봇 모델 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appointed-calibration",
   "metadata": {},
   "source": [
    "### 3.3.1 SmoothinFunction이 적용된 BLEU score 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "adequate-designer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원문: ['많', '은', '자연어', '처리', '연구자', '들', '이', '트랜스포머', '를', '선호', '한다']\n",
      "번역문: ['적', '은', '자연어', '학', '개발자', '들', '가', '트랜스포머', '을', '선호', '한다', '요']\n",
      "BLEU Score: 8.190757052088229e-155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import SmoothingFunction, sentence_bleu\n",
    "\n",
    "#!pip install nltk # nltk가 설치되어 있지 않은 경우 주석 해제\n",
    "reference = \"많 은 자연어 처리 연구자 들 이 트랜스포머 를 선호 한다\".split()\n",
    "candidate = \"적 은 자연어 학 개발자 들 가 트랜스포머 을 선호 한다 요\".split()\n",
    "\n",
    "print(\"원문:\", reference)\n",
    "print(\"번역문:\", candidate)\n",
    "print(\"BLEU Score:\", sentence_bleu([reference], candidate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "unknown-royalty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.5\n",
      "BLEU-2: 0.18181818181818182\n",
      "BLEU-3: 0.010000000000000004\n",
      "BLEU-4: 0.011111111111111112\n",
      "\n",
      "BLEU-Total: 0.05637560315259291\n"
     ]
    }
   ],
   "source": [
    "def calculate_bleu(reference, candidate, weights=[0.25, 0.25, 0.25, 0.25]):\n",
    "    return sentence_bleu([reference],\n",
    "                         candidate,\n",
    "                         weights=weights,\n",
    "                         smoothing_function=SmoothingFunction().method1)  # smoothing_function 적용\n",
    "\n",
    "print(\"BLEU-1:\", calculate_bleu(reference, candidate, weights=[1, 0, 0, 0]))\n",
    "print(\"BLEU-2:\", calculate_bleu(reference, candidate, weights=[0, 1, 0, 0]))\n",
    "print(\"BLEU-3:\", calculate_bleu(reference, candidate, weights=[0, 0, 1, 0]))\n",
    "print(\"BLEU-4:\", calculate_bleu(reference, candidate, weights=[0, 0, 0, 1]))\n",
    "\n",
    "print(\"\\nBLEU-Total:\", calculate_bleu(reference, candidate))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "growing-filing",
   "metadata": {},
   "source": [
    "### 3.3.2 모델 평가 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "found-production",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def evaluate(sentence, model, mecab, word_index, index_word):\n",
    "    if type(sentence) == str:        \n",
    "        sentence = utils.text_prep(sentence)\n",
    "\n",
    "        pieces = mecab.morphs(sentence)\n",
    "        tokens = [word_index[piece] for piece in pieces]\n",
    "        _input = tf.keras.preprocessing.sequence.pad_sequences([tokens],\n",
    "                                                               maxlen=enc_train.shape[-1],\n",
    "                                                               padding='post')\n",
    "    else:\n",
    "        pieces = [index_word[value] for value in sentence] # id 리스트로된 입력문장을 단어 리스트로 변환\n",
    "        _input = np.expand_dims(sentence, axis = 0) # str인 경우와 형태를 맞추기 위해서 차원을 늘림\n",
    "    ids = []\n",
    "    output = tf.expand_dims([word_index[\"<sos>\"]], 0)\n",
    "    \n",
    "    bleu_list = []\n",
    "    \n",
    "    for i in range(dec_train.shape[-1]):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = \\\n",
    "        generate_masks(_input, output)\n",
    "\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns =\\\n",
    "        transformer(_input, \n",
    "                    output,\n",
    "                    enc_padding_mask,\n",
    "                    combined_mask,\n",
    "                    dec_padding_mask)\n",
    "\n",
    "        predicted_id = \\\n",
    "        tf.argmax(tf.math.softmax(predictions, axis=-1)[0, -1]).numpy().item()  # predictions에 소프트맥스 함수를 적용하여 가장 큰 값의 인덱스를 predicted_id로 저장합니다.\n",
    "        \n",
    "        \n",
    "        \n",
    "        if word_index[\"<eos>\"] == predicted_id:\n",
    "            result = [index_word[value] for value in ids]    # 숫자를 문자열로 복원합니다.  \n",
    "            return pieces, result, enc_attns, dec_attns, dec_enc_attns\n",
    "        \n",
    "\n",
    "        ids.append(predicted_id)\n",
    "        output = tf.concat([output, tf.expand_dims([predicted_id], 0)], axis=-1)\n",
    "    result = [index_word[value] for value in ids]  # 단어 리스트로 예측 결과물 출력\n",
    "    \n",
    "    \n",
    "    return pieces, result, enc_attns, dec_attns, dec_enc_attns, bleu_list\n",
    "\n",
    "def translate(sentence, model, mecab, word_index, index_word):\n",
    "    pieces, result, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "    evaluate(sentence, model, mecab, word_index, index_word)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "studied-service",
   "metadata": {},
   "source": [
    "### 3.3.3 모델 BLEU 점수로 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "imposed-rubber",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_bleu(src_corpus, tgt_corpus, verbose=True):\n",
    "    total_score = 0.0\n",
    "    sample_size = len(tgt_corpus)\n",
    "\n",
    "    for idx in tqdm_notebook(range(sample_size)):\n",
    "        src_tokens = src_corpus[idx]\n",
    "        tgt_tokens = tgt_corpus[idx]\n",
    "        \n",
    "        src_sentence = [index_word[token] for token in src_tokens if token != 0]\n",
    "        src_sentence = \" \".join(src_sentence)\n",
    "        \n",
    "        reference = [index_word[token] for token in tgt_tokens  if token != 0]\n",
    "        reference = \" \".join(reference)\n",
    "        \n",
    "        candidate = translate(src_tokens, transformer, mecab, word_index, index_word)\n",
    "        candidate = \" \".join(candidate)\n",
    "        \n",
    "        \n",
    "        score = sentence_bleu([reference], candidate,\n",
    "                              smoothing_function=SmoothingFunction().method1)\n",
    "        total_score += score\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Source Sentence: \", src_sentence)\n",
    "            print(\"Model Prediction: \", candidate)\n",
    "            print(\"Real: \", reference)\n",
    "            print(\"Score: %lf\\n\" % score)\n",
    "\n",
    "    print(\"Num of Sample:\", sample_size)\n",
    "    print(\"Total Score:\", total_score / sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "swiss-vintage",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cd1ce2dbdfd4bf8aaca47d601367df1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Sentence:  3 박 4 일 정도 놀 러 가 고 싶 다\n",
      "Model Prediction:  여행 은 언제나 좋 죠\n",
      "Real:  <sos> 여행 은 언제나 좋 죠 <eos>\n",
      "Score: 0.367879\n",
      "\n",
      "Source Sentence:  sd 카드 망가졌 어\n",
      "Model Prediction:  다시 새로 사 는 게 마음 편해요\n",
      "Real:  <sos> 다시 새로 사 는 게 마음 편해요 <eos>\n",
      "Score: 0.513417\n",
      "\n",
      "Source Sentence:  가스 불 켜 고 나갔 어\n",
      "Model Prediction:  빨리 집 에 돌아가 서 부모 님 집 에 돌아가 서 쉬 는 건 어떨까 요\n",
      "Real:  <sos> 빨리 집 에 돌아가 서 끄 고 나오 세요 <eos>\n",
      "Score: 0.335733\n",
      "\n",
      "Num of Sample: 3\n",
      "Total Score: 0.405676454288949\n"
     ]
    }
   ],
   "source": [
    "eval_bleu(enc_val[:3], dec_val[:3], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pregnant-border",
   "metadata": {},
   "source": [
    "## 4. 결론"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reduced-claim",
   "metadata": {},
   "source": [
    "결과적으로 Transformer를 기반으로 한 챗봇모델의 평가 결과는 평균 BLEU-score가 0.406인 모델을 얻을 수가 있었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "broad-modification",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q : 배가 고프네\n",
      "A : 저도요!\n",
      "\n",
      "\n",
      "Q : 전화 받아!\n",
      "A : 연락이많이되겠네요\n"
     ]
    }
   ],
   "source": [
    "sentnece1 = \"배가 고프네\"\n",
    "answer1 = \"\".join(translate(sentnece1, transformer, mecab, word_index, index_word))\n",
    "print(f\"Q : {sentnece1}\")\n",
    "print(f\"A : {answer1}\")\n",
    "print(\"\\n\")\n",
    "sentnece1 = \"전화 받아!\"\n",
    "answer1 = \"\".join(translate(sentnece1, transformer, mecab, word_index, index_word))\n",
    "print(f\"Q : {sentnece1}\")\n",
    "print(f\"A : {answer1}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
