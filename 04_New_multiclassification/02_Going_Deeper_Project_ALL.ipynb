{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "light-virgin",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import reuters\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 벡터화 함수\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "# 머신러닝 모델들\n",
    "from sklearn.naive_bayes import MultinomialNB #다항분포 나이브 베이즈 모델\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# 모델 검증\n",
    "from sklearn.metrics import accuracy_score #정확도 계산\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generous-cancer",
   "metadata": {},
   "source": [
    "## 2. 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "virgin-jurisdiction",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 샘플의 수: 8982\n",
      "테스트 샘플의 수: 2246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/datasets/reuters.py:148: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/datasets/reuters.py:149: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    }
   ],
   "source": [
    "num_words = None\n",
    "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=num_words, test_split=0.2)\n",
    "print('훈련 샘플의 수: {}'.format(len(x_train)))\n",
    "print('테스트 샘플의 수: {}'.format(len(x_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quantitative-calcium",
   "metadata": {},
   "source": [
    "## 3. 데이터 살펴보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "reverse-complement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 27595, 28842, 8, 43, 10, 447, 5, 25, 207, 270, 5, 3095, 111, 16, 369, 186, 90, 67, 7, 89, 5, 19, 102, 6, 19, 124, 15, 90, 67, 84, 22, 482, 26, 7, 48, 4, 49, 8, 864, 39, 209, 154, 6, 151, 6, 83, 11, 15, 22, 155, 11, 15, 7, 48, 9, 4579, 1005, 504, 6, 258, 6, 272, 11, 15, 22, 134, 44, 11, 15, 16, 8, 197, 1245, 90, 67, 52, 29, 209, 30, 32, 132, 6, 109, 15, 17, 12]\n",
      "3\n",
      "(8982,) (8982,)\n",
      "\n",
      "\n",
      "[1, 4, 1378, 2025, 9, 697, 4622, 111, 8, 25, 109, 29, 3650, 11, 150, 244, 364, 33, 30, 30, 1398, 333, 6, 18292, 159, 9, 1084, 363, 13, 19231, 71, 9, 16273, 71, 117, 4, 225, 78, 206, 10, 9, 1214, 8, 4, 270, 5, 16273, 7, 748, 48, 9, 19231, 7, 207, 1451, 966, 1864, 793, 97, 133, 336, 7, 4, 493, 98, 273, 104, 284, 25, 39, 338, 22, 905, 220, 3465, 644, 59, 20, 6, 119, 61, 11, 15, 58, 579, 26, 10, 67, 7, 4, 738, 98, 43, 88, 333, 722, 12, 20, 6, 19, 746, 35, 15, 10, 9, 1214, 855, 129, 783, 21, 4, 2280, 244, 364, 51, 16, 299, 452, 16, 515, 4, 99, 29, 5, 4, 364, 281, 48, 10, 9, 1214, 23, 644, 47, 20, 324, 27, 56, 23406, 28185, 5, 192, 510, 17, 12]\n",
      "3\n",
      "(2246,) (2246,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0])\n",
    "print(y_train[0])\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(\"\\n\")\n",
    "print(x_test[0])\n",
    "print(y_test[0])\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fifty-vision",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "클래스의 수 : 46\n"
     ]
    }
   ],
   "source": [
    "num_classes = max(y_train) + 1\n",
    "print('클래스의 수 : {}'.format(num_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "rental-judges",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련용 뉴스의 최대 길이 :2376\n",
      "훈련용 뉴스의 평균 길이 :145.5398574927633\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEGCAYAAACUzrmNAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAAZuUlEQVR4nO3df7RldXnf8ffHEdBGGoZAWMgPB3WSqI0SvCpZoSlqBcS0aGsU24QRiUQLEVu1GaIVNGUFmqipJiEOgThaI2VFDVOh4kggxvqDGXAEBkIYBcpMEEZRfmhEgad/7O+tx8u9s8/cmXPvufe+X2vtdfZ59o/z7MO587D3/u7vN1WFJEk78rj5TkCSNP4sFpKkXhYLSVIvi4UkqZfFQpLU6/HzncAo7LfffrVixYr5TkOSFpRrr732m1W1/3TLFmWxWLFiBRs3bpzvNCRpQUlyx0zLRnYZKskTklyT5KtJNid5V4sfluTLSbYk+Z9J9mzxvdr7LW35ioF9ndnityQ5dlQ5S5KmN8p7Fg8BL6qq5wCHA8clORI4D3hfVT0d+DZwSlv/FODbLf6+th5JngmcCDwLOA74kyTLRpi3JGmKkRWL6jzY3u7RpgJeBPxli68FXt7mT2jvactfnCQtfnFVPVRVtwFbgOePKm9J0mONtDVUkmVJNgH3AOuBrwHfqaqH2ypbgYPa/EHAnQBt+X3ATw3Gp9lm8LNOTbIxycbt27eP4GgkaekaabGoqkeq6nDgYLqzgZ8b4WetqaqJqprYf/9pb+ZLkmZpTp6zqKrvAFcBvwjsk2SyFdbBwLY2vw04BKAt/0ngW4PxabaRJM2BUbaG2j/JPm3+icBLgJvpisYr22qrgEvb/Lr2nrb8r6vrEncdcGJrLXUYsBK4ZlR5S5Iea5TPWRwIrG0tlx4HXFJVn0pyE3Bxkv8KfAW4sK1/IfCRJFuAe+laQFFVm5NcAtwEPAycVlWPjDBvSdIUWYzjWUxMTJQP5UnSzklybVVNTLdsUT7BPSorVl82bfz2c182x5lI0tyyI0FJUi+LhSSpl8VCktTLYiFJ6mWxkCT1slhIknpZLCRJvSwWkqReFgtJUi+LhSSpl8VCktTLYiFJ6mWxkCT1slhIknpZLCRJvSwWkqReFgtJUi+LhSSpl8VCktTLYiFJ6mWxkCT1slhIknpZLCRJvSwWkqReFgtJUq+RFYskhyS5KslNSTYnOaPFz06yLcmmNh0/sM2ZSbYkuSXJsQPx41psS5LVo8pZkjS9x49w3w8Db6mq65LsDVybZH1b9r6q+oPBlZM8EzgReBbwZOCzSX6mLf5j4CXAVmBDknVVddMIc5ckDRhZsaiqu4C72vwDSW4GDtrBJicAF1fVQ8BtSbYAz2/LtlTV1wGSXNzWtVhI0hyZk3sWSVYAvwB8uYVOT3J9kouSLG+xg4A7Bzbb2mIzxad+xqlJNibZuH379t19CJK0pI28WCR5EvBx4M1VdT9wPvA04HC6M4/37I7Pqao1VTVRVRP777//7tilJKkZ5T0LkuxBVyg+WlWfAKiquweWXwB8qr3dBhwysPnBLcYO4pKkOTDK1lABLgRurqr3DsQPHFjtFcCNbX4dcGKSvZIcBqwErgE2ACuTHJZkT7qb4OtGlbck6bFGeWbxS8CvAzck2dRivwO8JsnhQAG3A78JUFWbk1xCd+P6YeC0qnoEIMnpwBXAMuCiqto8wrwlSVOMsjXU54FMs+jyHWxzDnDONPHLd7SdJGm0fIJbktTLYiFJ6mWxkCT1slhIknpZLCRJvSwWkqReFgtJUi+LhSSpl8VCktRrpB0JLlQrVl823ylI0ljxzEKS1MtiIUnqZbGQJPWyWEiSelksJEm9LBaSpF4WC0lSr95ikeRXk+zd5t+R5BNJjhh9apKkcTHMmcV/qaoHkhwF/EvgQuD80aYlSRonwxSLR9rry4A1VXUZsOfoUpIkjZthisW2JB8EXg1cnmSvIbeTJC0Sw/yj/yrgCuDYqvoOsC/wtlEmJUkaL73Foqq+B9wDHNVCDwO3jjIpSdJ4GaY11FnAbwNnttAewP8YZVKSpPEyzGWoVwD/GvguQFX9A7D3KJOSJI2XYYrFD6qqgAJI8hOjTUmSNG6GKRaXtNZQ+yR5PfBZ4ILRpiVJGifD3OD+A+AvgY8DPwu8s6o+0LddkkOSXJXkpiSbk5zR4vsmWZ/k1va6vMWT5P1JtiS5fvAp8SSr2vq3Jlk124OVJM3OUMOqVtV6YP1O7vth4C1VdV3rLuTaJOuB1wJXVtW5SVYDq+luoL8UWNmmF9A9Jf6CJPsCZwETdJfCrk2yrqq+vZP5SJJmacYziyQPJLl/mumBJPf37biq7qqq69r8A8DNwEHACcDattpa4OVt/gTgw9X5Et1lrwOBY4H1VXVvKxDrgeNmd7iSpNmY8cyiqnZbi6ckK4BfAL4MHFBVd7VF3wAOaPMHAXcObLa1xWaKT/2MU4FTAQ499NDdlbokiSEvQ7X7B0fRXQb6fFV9ZdgPSPIkuvsdb66q+5P8/2VVVUlq51KeXlWtAdYATExM7JZ9SpI6wzyU9066y0U/BewHfCjJO4bZeZI96ArFR6vqEy18d7u8RHu9p8W3AYcMbH5wi80UlyTNkWGazv574HlVdVZVnQUcCfx630bpTiEuBG6uqvcOLFoHTLZoWgVcOhA/qbWKOhK4r12uugI4Jsny1nLqmBaTJM2RYS5D/QPwBOD77f1eDPd/9r9EV1RuSLKpxX4HOJfu2Y1TgDvoOioEuBw4HtgCfA84GaCq7k3yu8CGtt67q+reIT5fkrSbDFMs7gM2t2avBbwEuCbJ+wGq6k3TbVRVnwcy3TLgxdOsX8BpM+zrIuCiIXKVJI3AMMXik22adPVoUpEkjaveYlFVa/vWkSQtbsO0hvqVJF9Jcu/OPJQnSVo8hrkM9YfAvwFuaPcVJElLzDBNZ+8EbrRQSNLSNcyZxX8GLk/yN8BDk8Epz05IkhaxYYrFOcCDdM9a7DnadCRJ42iYYvHkqvpnI89EkjS2hrlncXmSY0aeiSRpbA1TLN4IfDrJP9p0VpKWpmEeyttt41pIkhamYcezWE433OkTJmNV9blRJSVJGi+9xSLJbwBn0I0jsYmui/IvAi8aaWaSpLExzD2LM4DnAXdU1Qvphkf9ziiTkiSNl2GKxfer6vsASfaqqr8Dfna0aUmSxskw9yy2JtkH+CtgfZJv0w1aJElaIoZpDfWKNnt2kquAnwQ+PdKsJEljZZguyp+WZK/Jt8AK4J+MMilJ0ngZ5p7Fx4FHkjwdWAMcAvzFSLOSJI2VYYrFo1X1MPAK4ANV9TbgwNGmJUkaJ8MUix8meQ2wCvhUi+0xupQkSeNmmGJxMvCLwDlVdVuSw4CPjDYtSdI4GaY11E3Amwbe3wacN8qkJEnjZZgzC0nSEmexkCT1mrFYJPlIez1j7tKRJI2jHZ1ZPDfJk4HXJVmeZN/BqW/HSS5Kck+SGwdiZyfZlmRTm44fWHZmki1Jbkly7ED8uBbbkmT1bA9UkjR7O7rB/afAlcBTgWvpnt6eVC2+Ix8C/gj48JT4+6rqDwYDSZ4JnAg8C3gy8NkkP9MW/zHwEmArsCHJunbTXZI0R2Y8s6iq91fVM4CLquqpVXXYwNRXKCYHR7p3yDxOAC6uqodaa6stwPPbtKWqvl5VPwAubutKkuZQ7w3uqnpjkuckOb1Nz97Fzzw9yfXtMtXyFjsIuHNgna0tNlP8MZKcmmRjko3bt2/fxRQlSYOG6UjwTcBHgZ9u00eT/NYsP+984GnA4cBdwHtmuZ/HqKo1VTVRVRP777//7tqtJInhxrP4DeAFVfVdgCTn0Q2r+oGd/bCquntyPskF/Kj7kG10HRROOrjF2EFckjRHhnnOIsAjA+8f4cdvdg8tyWAHhK8AJltKrQNOTLJX605kJXANsAFYmeSwJHvS3QRfN5vPliTN3jBnFn8OfDnJJ9v7lwMX9m2U5GPA0cB+SbYCZwFHJzmcrjXV7cBvAlTV5iSXADcBDwOnVdUjbT+nA1cAy+hutm8e8tgkSbvJMH1DvTfJ1cBRLXRyVX1liO1eM014xiJTVecA50wTvxy4vO/zJEmjM8yZBVV1HXDdiHORJI0p+4aSJPWyWEiSeu2wWCRZluSquUpGkjSedlgsWoukR5P85BzlI0kaQ8Pc4H4QuCHJeuC7k8GqetPMm0iSFpNhisUn2iRJWqKGec5ibZInAodW1S1zkJMkacwM05HgvwI2AZ9u7w9PYpcbkrSEDNN09my6cSW+A1BVm+gf+EiStIgMUyx+WFX3TYk9OopkJEnjaZgb3JuT/DtgWZKVwJuAL4w2LUnSOBnmzOK36MbGfgj4GHA/8OYR5iRJGjPDtIb6HvD2NuhRVdUDo09LkjROhmkN9bwkNwDX0z2c99Ukzx19apKkcTHMPYsLgf9QVX8LkOQougGRnj3KxCRJ42OYexaPTBYKgKr6PN1odpKkJWLGM4skR7TZv0nyQbqb2wW8Grh69KlJksbFji5DvWfK+7MG5msEuUiSxtSMxaKqXjiXiUiSxlfvDe4k+wAnASsG17eLcklaOoZpDXU58CXgBuzmQ5KWpGGKxROq6j+NPBNJ0tgaplh8JMnrgU/RdfkBQFXdO7KsFpgVqy+bNn77uS+b40wkaTSGKRY/AH4feDs/agVV2E25JC0ZwxSLtwBPr6pvjjoZSdJ4GuYJ7i3A90adiCRpfA1TLL4LbErywSTvn5z6NkpyUZJ7ktw4ENs3yfokt7bX5S2ett8tSa4feHqcJKva+rcmWTWbg5Qk7ZphisVfAefQDXh07cDU50PAcVNiq4Erq2olcGV7D/BSYGWbTgXOh6640D05/gK6oV3PmiwwkqS5M8x4Fmtns+Oq+lySFVPCJwBHt/m1dH1M/XaLf7iqCvhSkn2SHNjWXT/Z8irJeroC9LHZ5CRJmp1hnuC+jWn6gqqq2bSGOqCq7mrz3wAOaPMHAXcOrLe1xWaKT5fnqXRnJRx66KGzSE2SNJNhWkNNDMw/AfhVYN9d/eCqqiS7rUPCqloDrAGYmJiwo0NJ2o1671lU1bcGpm1V9YfAbJ82u7tdXqK93tPi24BDBtY7uMVmikuS5tAww6oeMTBNJHkDw52RTGcdMNmiaRVw6UD8pNYq6kjgvna56grgmCTL243tY1pMkjSHhvlHf3Bci4eB24FX9W2U5GN0N6j3S7KVrlXTucAlSU4B7hjYz+XA8fzomY6ToetSJMnvAhvaeu+2mxFJmnvDtIaa1bgWVfWaGRa9eJp1Czhthv1cBFw0mxwkSbvHMK2h9gL+LY8dz+Ldo0tLkjROhrkMdSlwH92DeA/1rCtJWoSGKRYHV9XUJ7ElSUvIMN19fCHJz488E0nS2BrmzOIo4LXtSe6HgNDdk372SDOTJI2NYYrFS0eehSRprA3TdPaOuUhkMXK4VUmLxTD3LCRJS5zFQpLUy2IhSeplsZAk9bJYSJJ6WSwkSb0sFpKkXhYLSVIvi4UkqZfFQpLUy2IhSeplsZAk9bJYSJJ6WSwkSb0sFpKkXhYLSVIvi4UkqZfFQpLUy2IhSeo1L8Uiye1JbkiyKcnGFts3yfokt7bX5S2eJO9PsiXJ9UmOmI+cJWkpm88zixdW1eFVNdHerwaurKqVwJXtPcBLgZVtOhU4f84zlaQlbpwuQ50ArG3za4GXD8Q/XJ0vAfskOXAe8pOkJWu+ikUBn0lybZJTW+yAqrqrzX8DOKDNHwTcObDt1hb7MUlOTbIxycbt27ePKm9JWpIeP0+fe1RVbUvy08D6JH83uLCqKkntzA6rag2wBmBiYmKntp1rK1ZfNm389nNfNseZSNJw5uXMoqq2tdd7gE8Czwfunry81F7vaatvAw4Z2PzgFpMkzZE5LxZJfiLJ3pPzwDHAjcA6YFVbbRVwaZtfB5zUWkUdCdw3cLlKkjQH5uMy1AHAJ5NMfv5fVNWnk2wALklyCnAH8Kq2/uXA8cAW4HvAyXOfsiQtbXNeLKrq68Bzpol/C3jxNPECTpuD1CRJMxinprOSpDFlsZAk9bJYSJJ6WSwkSb0sFpKkXhYLSVKv+eruQ9OwGxBJ48ozC0lSL4uFJKmXxUKS1MtiIUnqZbGQJPWyNdQCYCspSfPNMwtJUi+LhSSpl8VCktTLYiFJ6mWxkCT1sjXUAmYrKUlzxTMLSVIvi4UkqZeXoZaQmS5bgZeuJO2YxWIR2lFRkKTZ8DKUJKmXZxYCbFklaccsFpoVi4u0tFgstEO76/6HxUVa2BZMsUhyHPDfgWXAn1XVufOckqbhzXVpcVoQxSLJMuCPgZcAW4ENSdZV1U3zm5l21c4WF89EpPmxIIoF8HxgS1V9HSDJxcAJgMViibG4SPNjoRSLg4A7B95vBV4wuEKSU4FT29sHk9wyi8/ZD/jmrDJcHBbd8ee8nd5k0X0HO2mpHz8s7e/gKTMtWCjFoldVrQHW7Mo+kmysqondlNKCs9SPH/wOlvrxg9/BTBbKQ3nbgEMG3h/cYpKkObBQisUGYGWSw5LsCZwIrJvnnCRpyVgQl6Gq6uEkpwNX0DWdvaiqNo/go3bpMtYisNSPH/wOlvrxg9/BtFJV852DJGnMLZTLUJKkeWSxkCT1sljQdSWS5JYkW5Ksnu98RinJ7UluSLIpycYW2zfJ+iS3ttflLZ4k72/fy/VJjpjf7HdekouS3JPkxoHYTh9vklVt/VuTrJqPY5mtGb6Ds5Nsa7+DTUmOH1h2ZvsObkly7EB8Qf6dJDkkyVVJbkqyOckZLb6kfge7rKqW9ER3w/xrwFOBPYGvAs+c77xGeLy3A/tNif03YHWbXw2c1+aPB/43EOBI4Mvznf8sjveXgSOAG2d7vMC+wNfb6/I2v3y+j20Xv4OzgbdOs+4z29/AXsBh7W9j2UL+OwEOBI5o83sDf9+Oc0n9DnZ18sxioCuRqvoBMNmVyFJyArC2za8FXj4Q/3B1vgTsk+TAechv1qrqc8C9U8I7e7zHAuur6t6q+jawHjhu5MnvJjN8BzM5Abi4qh6qqtuALXR/Iwv276Sq7qqq69r8A8DNdL1CLKnfwa6yWEzflchB85TLXCjgM0mubV2kABxQVXe1+W8AB7T5xfrd7OzxLtbv4fR2meWiyUswLPLvIMkK4BeAL+PvYKdYLJaeo6rqCOClwGlJfnlwYXXn20umPfVSO94B5wNPAw4H7gLeM6/ZzIEkTwI+Dry5qu4fXLaEfwdDs1gssa5Eqmpbe70H+CTd5YW7Jy8vtdd72uqL9bvZ2eNddN9DVd1dVY9U1aPABXS/A1ik30GSPegKxUer6hMtvOR/BzvDYrGEuhJJ8hNJ9p6cB44BbqQ73smWHauAS9v8OuCk1jrkSOC+gdP2hWxnj/cK4Jgky9vlmmNabMGacu/pFXS/A+i+gxOT7JXkMGAlcA0L+O8kSYALgZur6r0Di5b872CnzPcd9nGY6Fo//D1da4+3z3c+IzzOp9K1YvkqsHnyWIGfAq4EbgU+C+zb4qEbdOprwA3AxHwfwyyO+WN0l1l+SHeN+ZTZHC/wOrqbvVuAk+f7uHbDd/CRdozX0/3jeODA+m9v38EtwEsH4gvy7wQ4iu4S0/XApjYdv9R+B7s62d2HJKmXl6EkSb0sFpKkXhYLSVIvi4UkqZfFQpLUy2KhBS/JgyPY5+FTemI9O8lbd2F/v5rk5iRX7Z4MZ53H7Un2m88ctDBZLKTpHU7XFn93OQV4fVW9cDfuU5ozFgstKknelmRD6yDvXS22ov1f/QVtPIPPJHliW/a8tu6mJL+f5Mb2hPK7gVe3+Kvb7p+Z5OokX0/yphk+/zXpxgu5Mcl5LfZOugfDLkzy+1PWPzDJ59rn3Jjkn7f4+Uk2tnzfNbD+7Ul+r62/MckRSa5I8rUkb2jrHN32eVm68Sf+NMlj/taT/FqSa9q+PphkWZs+1HK5Icl/3MX/JFos5vupQCenXZ2AB9vrMcAauidwHwd8im4shxXAw8Dhbb1LgF9r8zcCv9jmz6WN+QC8Fvijgc84G/gC3TgP+wHfAvaYkseTgf8L7A88Hvhr4OVt2dVM8wQ88BZ+9CT9MmDvNr/vQOxq4Nnt/e3AG9v8++ieSt67febdLX408H26J/aX0XWl/cqB7fcDngH8r8ljAP4EOAl4Ll033JP57TPf/32dxmPyzEKLyTFt+gpwHfBzdH0bAdxWVZva/LXAiiT70P3j/MUW/4ue/V9W3TgP36TrdO6AKcufB1xdVdur6mHgo3TFakc2ACcnORv4+erGWwB4VZLr2rE8i26wnkmTfTLdQDcwzwNVtR14qB0TwDXVjT3xCF13H0dN+dwX0xWGDUk2tfdPpRvQ56lJPpDkOOB+JLr/+5EWiwC/V1Uf/LFgN4bBQwOhR4AnzmL/U/exy38/VfW51k38y4APJXkv8LfAW4HnVdW3k3wIeMI0eTw6JadHB3Ka2o/P1PcB1lbVmVNzSvIcuoF+3gC8iq4/JC1xnlloMbkCeF0bt4AkByX56ZlWrqrvAA8keUELnTiw+AG6yzs74xrgXyTZL8ky4DXA3+xogyRPobt8dAHwZ3TDn/5T4LvAfUkOoBt7ZGc9v/UQ+zjg1cDnpyy/Enjl5PeTbjzqp7SWUo+rqo8D72j5SJ5ZaPGoqs8keQbwxa5Xah4Efo3uLGAmpwAXJHmU7h/2+1r8KmB1u0Tze0N+/l1JVrdtQ3fZ6tKezY4G3pbkhy3fk6rqtiRfAf6ObmS2/zPM50+xAfgj4Oktn09OyfWmJO+gGzXxcXQ90p4G/CPw5wM3xB9z5qGlyV5ntaQleVJVPdjmV9N11X3GPKe1S5IcDby1qn5lnlPRIuKZhZa6lyU5k+5v4Q66VlCSpvDMQpLUyxvckqReFgtJUi+LhSSpl8VCktTLYiFJ6vX/AHunIAk82uh/AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('훈련용 뉴스의 최대 길이 :{}'.format(max(len(l) for l in x_train)))\n",
    "print('훈련용 뉴스의 평균 길이 :{}'.format(sum(map(len, x_train))/len(x_train)))\n",
    "\n",
    "plt.hist([len(s) for s in x_train], bins=50)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "floating-compilation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 792x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axe = plt.subplots(ncols=1)\n",
    "fig.set_size_inches(11,5)\n",
    "sns.countplot(x=y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "wicked-james",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>label</th>\n",
       "      <th>len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[1, 27595, 28842, 8, 43, 10, 447, 5, 25, 207, ...</td>\n",
       "      <td>3</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[1, 3267, 699, 3434, 2295, 56, 16784, 7511, 9,...</td>\n",
       "      <td>4</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[1, 53, 12, 284, 15, 14, 272, 26, 53, 959, 32,...</td>\n",
       "      <td>3</td>\n",
       "      <td>139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[1, 4, 686, 867, 558, 4, 37, 38, 309, 2276, 46...</td>\n",
       "      <td>4</td>\n",
       "      <td>224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[1, 8295, 111, 8, 25, 166, 40, 638, 10, 436, 2...</td>\n",
       "      <td>4</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8977</th>\n",
       "      <td>[1, 313, 262, 2529, 1426, 8, 130, 40, 129, 363...</td>\n",
       "      <td>19</td>\n",
       "      <td>197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8978</th>\n",
       "      <td>[1, 4, 96, 5, 340, 3976, 23, 328, 6, 154, 7, 4...</td>\n",
       "      <td>19</td>\n",
       "      <td>214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8979</th>\n",
       "      <td>[1, 141, 3890, 387, 81, 8, 16, 1629, 10, 340, ...</td>\n",
       "      <td>25</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8980</th>\n",
       "      <td>[1, 53, 46, 957, 26, 14, 74, 132, 26, 39, 46, ...</td>\n",
       "      <td>3</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8981</th>\n",
       "      <td>[1, 227, 2406, 91, 21969, 125, 2855, 21, 4, 39...</td>\n",
       "      <td>25</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8982 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               features  label  len\n",
       "0     [1, 27595, 28842, 8, 43, 10, 447, 5, 25, 207, ...      3   87\n",
       "1     [1, 3267, 699, 3434, 2295, 56, 16784, 7511, 9,...      4   56\n",
       "2     [1, 53, 12, 284, 15, 14, 272, 26, 53, 959, 32,...      3  139\n",
       "3     [1, 4, 686, 867, 558, 4, 37, 38, 309, 2276, 46...      4  224\n",
       "4     [1, 8295, 111, 8, 25, 166, 40, 638, 10, 436, 2...      4  101\n",
       "...                                                 ...    ...  ...\n",
       "8977  [1, 313, 262, 2529, 1426, 8, 130, 40, 129, 363...     19  197\n",
       "8978  [1, 4, 96, 5, 340, 3976, 23, 328, 6, 154, 7, 4...     19  214\n",
       "8979  [1, 141, 3890, 387, 81, 8, 16, 1629, 10, 340, ...     25   77\n",
       "8980  [1, 53, 46, 957, 26, 14, 74, 132, 26, 39, 46, ...      3  119\n",
       "8981  [1, 227, 2406, 91, 21969, 125, 2855, 21, 4, 39...     25  105\n",
       "\n",
       "[8982 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.DataFrame({\"features\": x_train, \"label\":y_train})\n",
    "train_data[\"len\"] = train_data[\"features\"].apply(lambda x: len(x))\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "generic-chart",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# label별 평균 문장의 길이\n",
    "train_data.groupby(\"label\").mean().plot(kind = \"bar\")\n",
    "plt.ylabel(\"Average of length\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pacific-bahamas",
   "metadata": {},
   "source": [
    "## 3. 새롭게 벡터화시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "upper-surrey",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word_index = reuters.get_word_index(path=\"reuters_word_index.json\")\n",
    "index_to_word = { index+3 : word for word, index in word_index.items() }\n",
    "# index_to_word에 숫자 0은 <pad>, 숫자 1은 <sos>, 숫자 2는 <unk>를 넣어줍니다.\n",
    "for index, token in enumerate((\"<pad>\", \"<sos>\", \"<unk>\")):\n",
    "    index_to_word[index]=token\n",
    "\n",
    "x_train_word = []\n",
    "for sentence in x_train:\n",
    "    x_train_word += [' '.join([index_to_word[index] for index in sentence])]\n",
    "    \n",
    "x_test_word = []\n",
    "for sentence in x_test:\n",
    "    x_test_word += [' '.join([index_to_word[index] for index in sentence])]    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspected-sleeve",
   "metadata": {},
   "source": [
    "### 3.1 DTM -> TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "governing-tuner",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 데이터셋 DTM\n",
    "dtmvector = CountVectorizer()\n",
    "x_train_dtm = dtmvector.fit_transform(x_train_word)\n",
    "# 훈련 데이터셋 TF-IDF\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "tfidfv_train = tfidf_transformer.fit_transform(x_train_dtm)\n",
    "# 테스트 데이터셋 DTM\n",
    "x_test_dtm = dtmvector.transform(x_test_word)\n",
    "# 테스트 데이터셋 TF-IDF\n",
    "tfidfv_test = tfidf_transformer.transform(x_test_dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacterial-completion",
   "metadata": {},
   "source": [
    "## 4. 머신러닝 모델들 성능 비교하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surgical-silver",
   "metadata": {},
   "source": [
    "### 4.1 [MultinomialNB 모델](https://www.youtube.com/watch?v=3JWLIV3NaoQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impressed-replica",
   "metadata": {},
   "source": [
    "[집합의 연산중 결합법칙을 발견한 사이트](https://deep-learning-study.tistory.com/419)  \n",
    "[나이브 베이즈 위키 백과](https://ko.wikipedia.org/wiki/%EB%82%98%EC%9D%B4%EB%B8%8C_%EB%B2%A0%EC%9D%B4%EC%A6%88_%EB%B6%84%EB%A5%98)\n",
    "\n",
    "- 교집합 : $P(A \\cap B) = P(A , B) = P(A \\lvert B) \\cdot P(B)$  \n",
    "    - 두 사건 $A, B$가 서로 독립인 경우 $P(A \\cap B) = P(A)\\cdot P(B)$ \n",
    "    - 두 사건 $A, B$가 서로 종속인 경우 $P(A \\cap B) = P(A \\lvert B) \\cdot P(B)$\n",
    "\n",
    "- 조건부 확률\n",
    "    - $P(A \\lvert B) = \\frac{P(A \\cap B)}{P(B)}$  \n",
    "- 베이즈 정리\n",
    "    - $P(A), P(B), P(B \\lvert A)$를 알고 있을 때, $P(A \\lvert B)$를 구할 수 있습니다.\n",
    "    - $P(A \\lvert B) = \\frac{P(B \\lvert A) \\cdot P(A)}{P(B)}$\n",
    "    - 예를 들어 스팸 메일을 분리하면서 \"free\"라는 단어가 자주 보여 해당 단어가 포함된 메일은 스팸으로 분리하려고 합니다.  \n",
    "    이때, \"free\"라는 단어가 포함하였는데 스팸메일로 분류되는 확률을 구하겠습니다.\n",
    "        - 사건 $A$가 전체 메일 중에서 \"free\" 라는 단어가 포함되어 있는 경우\n",
    "        - 사건 $B$가 전체 메일 중에서 스팸메일인 경우  \n",
    "        - $P(B|A) = \\frac{P(B) \\cdot P(A \\lvert B)}{P(A)}$\n",
    "        - $posterior = \\frac{prior \\times likelihood}{evidence}$\n",
    "        - 베이즈 정리는 사전 확률과 사후 확률의 관계를 나타내는 정리를 말합니다.\n",
    "\n",
    "해당 모델을 현재의 데이터셋에 적용한 경우 label의 클래스들을 $B_i$, featrues에 있는 단어들을 각각 $A_1, A_2 ... A_n$라고 하겠습니다.\n",
    "\n",
    "이때의 식은 $P(B_i \\lvert A_1, A_2 .... A_n)$로 정리가 가능하며 계산한 결과 가장 확률이 높게 나온 i번째 클래스로 분류를 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "absolute-hanging",
   "metadata": {},
   "source": [
    "#### 4.1.1 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "corrected-clinic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        12\n",
      "           1       0.79      0.21      0.33       105\n",
      "           2       0.00      0.00      0.00        20\n",
      "           3       0.72      0.92      0.81       813\n",
      "           4       0.45      0.96      0.61       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.00      0.00      0.00        14\n",
      "           7       0.00      0.00      0.00         3\n",
      "           8       0.00      0.00      0.00        38\n",
      "           9       0.00      0.00      0.00        25\n",
      "          10       0.00      0.00      0.00        30\n",
      "          11       0.80      0.29      0.42        83\n",
      "          12       0.00      0.00      0.00        13\n",
      "          13       0.00      0.00      0.00        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.75      0.18      0.29        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.00      0.00      0.00        20\n",
      "          19       0.73      0.58      0.64       133\n",
      "          20       0.00      0.00      0.00        70\n",
      "          21       0.00      0.00      0.00        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.00      0.00      0.00        12\n",
      "          24       0.00      0.00      0.00        19\n",
      "          25       0.00      0.00      0.00        31\n",
      "          26       0.00      0.00      0.00         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.00      0.00      0.00        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       0.00      0.00      0.00        10\n",
      "          33       0.00      0.00      0.00         5\n",
      "          34       0.00      0.00      0.00         7\n",
      "          35       0.00      0.00      0.00         6\n",
      "          36       0.00      0.00      0.00        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.00      0.00      0.00         6\n",
      "          44       0.00      0.00      0.00         5\n",
      "          45       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.60      2246\n",
      "   macro avg       0.09      0.07      0.07      2246\n",
      "weighted avg       0.50      0.60      0.50      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "model = MultinomialNB()\n",
    "model.fit(tfidfv_train, y_train)\n",
    "\n",
    "predicted = model.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "print(classification_report(y_test, predicted)) #예측값과 실제값 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hawaiian-christian",
   "metadata": {},
   "source": [
    "### 4.2 ComplementNB 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "judicial-theory",
   "metadata": {},
   "source": [
    "ComplementNB 모델은 기존의 NB 모델에서 데이터의 label 불균형 문제를 개선시킨 모델로 각 label마다 가중치를 부여하여 불균형 문제를 개선한 모델입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "tamil-communications",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.50      0.63        12\n",
      "           1       0.63      0.88      0.73       105\n",
      "           2       0.91      0.50      0.65        20\n",
      "           3       0.87      0.91      0.89       813\n",
      "           4       0.75      0.93      0.83       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.92      0.86      0.89        14\n",
      "           7       1.00      0.67      0.80         3\n",
      "           8       0.43      0.08      0.13        38\n",
      "           9       0.81      0.88      0.85        25\n",
      "          10       0.96      0.73      0.83        30\n",
      "          11       0.55      0.67      0.61        83\n",
      "          12       0.00      0.00      0.00        13\n",
      "          13       0.62      0.54      0.58        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.50      0.11      0.18         9\n",
      "          16       0.67      0.77      0.71        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.65      0.55      0.59        20\n",
      "          19       0.55      0.80      0.65       133\n",
      "          20       0.89      0.23      0.36        70\n",
      "          21       0.84      0.59      0.70        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.71      0.42      0.53        12\n",
      "          24       0.50      0.11      0.17        19\n",
      "          25       0.83      0.61      0.70        31\n",
      "          26       1.00      0.88      0.93         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.33      0.10      0.15        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       1.00      0.31      0.47        13\n",
      "          32       1.00      0.80      0.89        10\n",
      "          33       1.00      0.80      0.89         5\n",
      "          34       1.00      0.71      0.83         7\n",
      "          35       1.00      0.17      0.29         6\n",
      "          36       0.00      0.00      0.00        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       1.00      0.33      0.50         3\n",
      "          39       1.00      0.20      0.33         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       0.67      0.25      0.36         8\n",
      "          42       1.00      0.33      0.50         3\n",
      "          43       1.00      0.17      0.29         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.76      2246\n",
      "   macro avg       0.62      0.42      0.46      2246\n",
      "weighted avg       0.75      0.76      0.73      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "cb = ComplementNB()\n",
    "cb.fit(tfidfv_train, y_train)\n",
    "\n",
    "predicted = cb.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "print(classification_report(y_test, predicted)) #예측값과 실제값 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regional-burns",
   "metadata": {},
   "source": [
    "### 4.3 Logistic Regression 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "responsible-estonia",
   "metadata": {},
   "source": [
    "로지스틱 회귀모델에 대한 정리는 해당 [링크](https://sda96.github.io/2021-10/classification_problem)에 정리하였습니다. 하지만 일반적인 로지스틱 회귀 모델은 이진 분류 모델이라고 알고 있는데 어떻게 다중 분류 문제를 해결할 수 있는지 알아보았습니다.\n",
    "\n",
    "sklearn 패키지의 로지스틱 회귀 모델은 다중 분류 문제가 들어오는 경우 OvR(One-vs-rest) 알고리즘을 사용합니다.\n",
    "- [sklearn LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)  \n",
    "- [다중 분류 알고리즘 OvR](https://brunch.co.kr/@linecard/482)\n",
    "\n",
    "OvR 알고리즘은 만약 파랑, 검정, 빨강이라는 클래스에서 파랑을 분류하는 경우 파랑 클래스의 데이터를 제외하고는 모두 똑같은 클래스라고 판단하여 결국 '파랑 클래스냐, 파랑 클래스가 아니냐.' 라는 이진 분류 문제로 바꾸어서 생각하며 이 방법을 파랑, 검정, 빨강에 모두 적용하여 결국에는 다중 분류 문제를 해결할 수 있게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "sufficient-peace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.67      0.76        12\n",
      "           1       0.75      0.80      0.77       105\n",
      "           2       0.70      0.70      0.70        20\n",
      "           3       0.93      0.93      0.93       813\n",
      "           4       0.81      0.87      0.84       474\n",
      "           5       1.00      0.20      0.33         5\n",
      "           6       0.93      1.00      0.97        14\n",
      "           7       1.00      0.67      0.80         3\n",
      "           8       0.68      0.71      0.69        38\n",
      "           9       0.81      0.88      0.85        25\n",
      "          10       0.93      0.87      0.90        30\n",
      "          11       0.66      0.73      0.70        83\n",
      "          12       0.57      0.31      0.40        13\n",
      "          13       0.61      0.62      0.61        37\n",
      "          14       0.67      1.00      0.80         2\n",
      "          15       0.71      0.56      0.63         9\n",
      "          16       0.71      0.77      0.74        99\n",
      "          17       0.67      0.50      0.57        12\n",
      "          18       0.76      0.65      0.70        20\n",
      "          19       0.69      0.70      0.69       133\n",
      "          20       0.60      0.49      0.54        70\n",
      "          21       0.63      0.81      0.71        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.69      0.75      0.72        12\n",
      "          24       0.62      0.53      0.57        19\n",
      "          25       0.92      0.74      0.82        31\n",
      "          26       1.00      0.88      0.93         8\n",
      "          27       1.00      0.25      0.40         4\n",
      "          28       0.75      0.30      0.43        10\n",
      "          29       0.57      1.00      0.73         4\n",
      "          30       0.89      0.67      0.76        12\n",
      "          31       0.75      0.46      0.57        13\n",
      "          32       1.00      0.80      0.89        10\n",
      "          33       0.80      0.80      0.80         5\n",
      "          34       1.00      0.29      0.44         7\n",
      "          35       1.00      0.33      0.50         6\n",
      "          36       0.42      0.45      0.43        11\n",
      "          37       0.50      0.50      0.50         2\n",
      "          38       0.50      0.33      0.40         3\n",
      "          39       0.50      0.40      0.44         5\n",
      "          40       1.00      0.30      0.46        10\n",
      "          41       0.83      0.62      0.71         8\n",
      "          42       1.00      1.00      1.00         3\n",
      "          43       0.86      1.00      0.92         6\n",
      "          44       0.67      0.80      0.73         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.81      2246\n",
      "   macro avg       0.76      0.64      0.67      2246\n",
      "weighted avg       0.81      0.81      0.81      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(C=10000, penalty='l2')\n",
    "lr.fit(tfidfv_train, y_train)\n",
    "\n",
    "predicted = lr.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "print(classification_report(y_test, predicted)) #예측값과 실제값 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weird-eclipse",
   "metadata": {},
   "source": [
    "### 4.4 선형 서포트 벡터 머신"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protected-indication",
   "metadata": {},
   "source": [
    "[![image](https://user-images.githubusercontent.com/51338268/146365205-69972aa6-98d5-45f3-95d4-a00cbe313ab4.png)](https://m.blog.naver.com/PostView.naver?isHttpsRedirect=true&blogId=gdpresent&logNo=221717231990)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liberal-victim",
   "metadata": {},
   "source": [
    "선형 서포트 벡터 분류기은 그림과 같이 데이터들을 2개의 클래스로 분류하는 결정경계를 찾으며 **단, Margin이 최대가 되는 결정경계를 찾아서 이진 분류를 시키는 알고리즘**입니다.\n",
    "\n",
    "결정경계는 두 클래스를 분류하는 선이 되며, Support Vectors는 각 클래스 별로 Margin에 가장 가까운 데이터를 의미합니다. 서포트 벡터 머신이 학습하는 과정에서 사용되는 데이터가 2개 뿐이기에 계산량이 상당히 적게 소모한다는 것을 알 수가 있습니다.\n",
    "\n",
    "하지만 종종 선형 구분이 되지 않는 문제가 발생하며 이 문제를 해결하기 위해서 더 높은 차원으로 매핑하여 분리하는 방법이 제안되었으며 해당 과정에서 늘어나는 계산량을 막기 위해서 커널함수를 정의한 SVM(Support Vector Machine) 구조를 설계하였습니다.\n",
    "- [SVM 위키 백과 설명](https://ko.wikipedia.org/wiki/%EC%84%9C%ED%8F%AC%ED%8A%B8_%EB%B2%A1%ED%84%B0_%EB%A8%B8%EC%8B%A0)\n",
    "- [SVM 설명](https://blog.naver.com/tjdudwo93/221051481147)\n",
    "\n",
    "SVM의 파라미터로 cost와 gamma가 존재합니다\n",
    "- cost : 결정경계의 margin의 간격을 결정하며 cost가 작으면 margin은 넓어지고 cost가 크면 margin은 좁아집니다.\n",
    "- gamma : train 데이터 하나당 영향을 끼치는 범위를 의미합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "employed-audit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.67      0.67        12\n",
      "           1       0.66      0.70      0.68       105\n",
      "           2       0.75      0.75      0.75        20\n",
      "           3       0.92      0.90      0.91       813\n",
      "           4       0.81      0.86      0.84       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.75      0.86      0.80        14\n",
      "           7       0.50      0.33      0.40         3\n",
      "           8       0.68      0.71      0.69        38\n",
      "           9       0.81      0.88      0.85        25\n",
      "          10       0.83      0.83      0.83        30\n",
      "          11       0.61      0.72      0.66        83\n",
      "          12       0.33      0.31      0.32        13\n",
      "          13       0.55      0.49      0.51        37\n",
      "          14       0.25      0.50      0.33         2\n",
      "          15       0.50      0.44      0.47         9\n",
      "          16       0.60      0.71      0.65        99\n",
      "          17       1.00      0.33      0.50        12\n",
      "          18       0.72      0.65      0.68        20\n",
      "          19       0.60      0.64      0.62       133\n",
      "          20       0.52      0.46      0.48        70\n",
      "          21       0.67      0.81      0.73        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.55      0.50      0.52        12\n",
      "          24       0.64      0.37      0.47        19\n",
      "          25       0.85      0.55      0.67        31\n",
      "          26       0.88      0.88      0.88         8\n",
      "          27       1.00      0.50      0.67         4\n",
      "          28       0.80      0.40      0.53        10\n",
      "          29       0.38      0.75      0.50         4\n",
      "          30       0.75      0.50      0.60        12\n",
      "          31       0.78      0.54      0.64        13\n",
      "          32       1.00      0.80      0.89        10\n",
      "          33       0.80      0.80      0.80         5\n",
      "          34       1.00      0.57      0.73         7\n",
      "          35       1.00      0.50      0.67         6\n",
      "          36       0.42      0.45      0.43        11\n",
      "          37       0.40      1.00      0.57         2\n",
      "          38       0.50      0.33      0.40         3\n",
      "          39       1.00      0.40      0.57         5\n",
      "          40       0.60      0.30      0.40        10\n",
      "          41       0.60      0.38      0.46         8\n",
      "          42       0.67      0.67      0.67         3\n",
      "          43       0.67      1.00      0.80         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       0.33      1.00      0.50         1\n",
      "\n",
      "    accuracy                           0.78      2246\n",
      "   macro avg       0.66      0.60      0.60      2246\n",
      "weighted avg       0.78      0.78      0.77      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/svm/_base.py:975: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "lsvc = LinearSVC(C=1000, penalty='l1', max_iter=500, dual=False)\n",
    "lsvc.fit(tfidfv_train, y_train)\n",
    "\n",
    "predicted = lsvc.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "print(classification_report(y_test, predicted)) #예측값과 실제값 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plastic-municipality",
   "metadata": {},
   "source": [
    "### 4.5 의사결정나무"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hungarian-violin",
   "metadata": {},
   "source": [
    "![image](https://user-images.githubusercontent.com/51338268/146504553-907c454d-ff00-404d-83c8-e6a5cf8439fd.png)\n",
    "\n",
    "\n",
    "의사결정나무는 특성, 변수마다 이진분류를 진행하여 마지막 끝노드에 있는 비율이 높은 클래스로 분류하는 방식의 분류 모델이기도 하고, 끝노드의 데이터들의 평균이나 중앙값을 사용한 예측 모델이기도 합니다.\n",
    "\n",
    "의사결정나무의 가장 기본적인 알고리즘인 ID3 알고리즘은 2가지 개념을 토대로 분류를 진행합니다.\n",
    "- Entropy가 작아지는 방향으로 알고리즘이 진행됨\n",
    "    - Entropy는 정보량의 기대값을 의미합니다.\n",
    "    - $H[Y] = -\\sum^{n}_{i=1} p_i \\log p_i$\n",
    "- Information gain이 가장 높은 특성부터 분류를 진행함\n",
    "    - information gain은 X라는 조건에 의해 확률 변수 Y의 엔트로피가 얼마나 감소하였는지를 나태나는 값입니다.\n",
    "    - $IG[Y, x] = H[Y] - H[Y \\lvert X]$\n",
    "\n",
    "분류는 정해진 Entropy의 수치나 나누어지는 속성을 모두 사용하거나 정해진 속성의 개수만큼 사용하면 멈추게 됩니다.\n",
    "\n",
    "참고사이트\n",
    "- [의사결정나무](https://datascienceschool.net/03%20machine%20learning/12.01%20%EC%9D%98%EC%82%AC%EA%B2%B0%EC%A0%95%EB%82%98%EB%AC%B4.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "configured-pound",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        12\n",
      "           1       0.69      0.43      0.53       105\n",
      "           2       0.75      0.45      0.56        20\n",
      "           3       0.94      0.85      0.89       813\n",
      "           4       0.40      0.89      0.55       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.00      0.00      0.00        14\n",
      "           7       0.00      0.00      0.00         3\n",
      "           8       0.00      0.00      0.00        38\n",
      "           9       1.00      0.16      0.28        25\n",
      "          10       0.89      0.80      0.84        30\n",
      "          11       0.58      0.60      0.59        83\n",
      "          12       0.00      0.00      0.00        13\n",
      "          13       0.00      0.00      0.00        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.61      0.83      0.70        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.00      0.00      0.00        20\n",
      "          19       0.67      0.41      0.50       133\n",
      "          20       0.83      0.07      0.13        70\n",
      "          21       0.00      0.00      0.00        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.00      0.00      0.00        12\n",
      "          24       0.67      0.11      0.18        19\n",
      "          25       0.60      0.19      0.29        31\n",
      "          26       0.00      0.00      0.00         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.50      0.10      0.17        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       0.00      0.00      0.00        10\n",
      "          33       1.00      0.80      0.89         5\n",
      "          34       0.00      0.00      0.00         7\n",
      "          35       0.00      0.00      0.00         6\n",
      "          36       0.00      0.00      0.00        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.00      0.00      0.00         6\n",
      "          44       0.00      0.00      0.00         5\n",
      "          45       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.62      2246\n",
      "   macro avg       0.22      0.15      0.15      2246\n",
      "weighted avg       0.62      0.62      0.58      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "tree = DecisionTreeClassifier(max_depth=10, random_state=0)\n",
    "tree.fit(tfidfv_train, y_train)\n",
    "\n",
    "predicted = tree.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "print(classification_report(y_test, predicted)) #예측값과 실제값 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compatible-calcium",
   "metadata": {},
   "source": [
    "### 4.6 랜덤포레스트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numerical-semiconductor",
   "metadata": {},
   "source": [
    "[![image](https://user-images.githubusercontent.com/51338268/146507957-8be96281-3b09-4f0e-9563-2ec66261948f.png)](https://www.tibco.com/reference-center/what-is-a-random-forest)\n",
    "랜덤포레스트는 기존의 의사결정나무를 기반으로 만들어진 알고리즘으로 다수의 의사결정나무 모델을 사용하여 해당 모델들의 결과를 투표하는 방식으로 데이터를 분류합니다.\n",
    "\n",
    "랜덤포레스트의 특성은 다음과 같습니다.\n",
    "- boosting\n",
    "    - boosting은 각각의 의사결정 나무 모델에 일부러 반복추출된 데이터를 넣어서 해당 데이터에 과대적합되어 편향된 모델들을 만듭니다.\n",
    "    - 특정 데이터에 편향됨으로써 그 데이터에 편향된 모델을 그 데이터에 대한 성능이 더 좋게 나올거라 기대합니다.\n",
    "- random selection\n",
    "    - 기존의 의사결정나무는 가장 좋은, information gain이 가장 높은 속성부터 분류를 하였습니다.\n",
    "    - 하지만 랜덤포레스트는 무작위 속성을 선택하여 분류합니다.\n",
    "- aggregating\n",
    "    - 다수의 의사결정나무 모델로부터 다양한 결과가 나왔을 텐데 그 결과들을 토대로 투표를 진행하여 가장 많이 나온 클래스로 분류합니다.\n",
    "    \n",
    "흔히 boosting과 aggregating을 더한 기법은 bagging이라고 부릅니다.\n",
    "\n",
    "랜덤포레스트는 각 변수들이 얼마만큼 종속변수에 영향을 주는가에 대한 측정이 가능하며 계산하는 방법은 다음 [링크](https://velog.io/@vvakki_/%EB%9E%9C%EB%8D%A4-%ED%8F%AC%EB%A0%88%EC%8A%A4%ED%8A%B8%EC%97%90%EC%84%9C%EC%9D%98-%EB%B3%80%EC%88%98-%EC%A4%91%EC%9A%94%EB%8F%84Variable-Importance-3%EA%B0%80%EC%A7%80)에 있습니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "collected-recipient",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.58      0.35        12\n",
      "           1       0.35      0.60      0.44       105\n",
      "           2       0.32      0.40      0.36        20\n",
      "           3       0.82      0.89      0.85       813\n",
      "           4       0.62      0.84      0.71       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.67      0.43      0.52        14\n",
      "           7       0.50      0.33      0.40         3\n",
      "           8       0.51      0.47      0.49        38\n",
      "           9       1.00      0.28      0.44        25\n",
      "          10       0.46      0.20      0.28        30\n",
      "          11       0.56      0.64      0.60        83\n",
      "          12       0.40      0.15      0.22        13\n",
      "          13       0.33      0.16      0.22        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.59      0.46      0.52        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.44      0.20      0.28        20\n",
      "          19       0.61      0.50      0.55       133\n",
      "          20       0.51      0.33      0.40        70\n",
      "          21       0.55      0.22      0.32        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.33      0.08      0.13        12\n",
      "          24       0.33      0.05      0.09        19\n",
      "          25       1.00      0.23      0.37        31\n",
      "          26       0.00      0.00      0.00         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.00      0.00      0.00        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       1.00      0.10      0.18        10\n",
      "          33       1.00      0.40      0.57         5\n",
      "          34       0.00      0.00      0.00         7\n",
      "          35       1.00      0.17      0.29         6\n",
      "          36       0.43      0.27      0.33        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       1.00      0.30      0.46        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.75      0.50      0.60         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.65      2246\n",
      "   macro avg       0.40      0.25      0.28      2246\n",
      "weighted avg       0.63      0.65      0.62      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "forest = RandomForestClassifier(n_estimators=5, random_state=0)\n",
    "forest.fit(tfidfv_train, y_train)\n",
    "\n",
    "predicted = forest.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "print(classification_report(y_test, predicted)) #예측값과 실제값 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "furnished-length",
   "metadata": {},
   "source": [
    "### 4.7 그래디언트 부스팅"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternative-threshold",
   "metadata": {},
   "source": [
    "- 부스팅은 약한 분류기, 모델을 결합하는데 일반적으로 의사결정나무를 사용하며 하나의 강한 분류기를 만드는 과정을 말합니다.\n",
    "\n",
    "- 그래디언트는 해당 앙상블 모델의 예측값과 실제값의 차이인 잔차, 손실함수를 줄여나가는 방향으로 학습을 진행합니다.\n",
    "\n",
    "- 예측 모델의 경우 손실함수로 RMSE를 사용할 수 있지만 분류 모델에서는 이진 교차 엔트로피나 교차 엔트로피를 사용할 수 있습니다.\n",
    "\n",
    "[참고사이트](https://bkshin.tistory.com/entry/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-15-Gradient-Boost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "respective-moment",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.50      0.55        12\n",
      "           1       0.81      0.71      0.76       105\n",
      "           2       0.58      0.70      0.64        20\n",
      "           3       0.87      0.91      0.89       813\n",
      "           4       0.78      0.86      0.82       474\n",
      "           5       1.00      0.20      0.33         5\n",
      "           6       0.77      0.71      0.74        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.60      0.63      0.62        38\n",
      "           9       0.91      0.80      0.85        25\n",
      "          10       0.79      0.77      0.78        30\n",
      "          11       0.61      0.65      0.63        83\n",
      "          12       0.50      0.46      0.48        13\n",
      "          13       0.48      0.32      0.39        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.25      0.11      0.15         9\n",
      "          16       0.72      0.71      0.71        99\n",
      "          17       0.83      0.42      0.56        12\n",
      "          18       0.59      0.50      0.54        20\n",
      "          19       0.71      0.64      0.67       133\n",
      "          20       0.64      0.41      0.50        70\n",
      "          21       0.61      0.63      0.62        27\n",
      "          22       0.33      0.14      0.20         7\n",
      "          23       0.62      0.67      0.64        12\n",
      "          24       0.69      0.47      0.56        19\n",
      "          25       0.83      0.65      0.73        31\n",
      "          26       1.00      1.00      1.00         8\n",
      "          27       0.33      0.50      0.40         4\n",
      "          28       0.25      0.20      0.22        10\n",
      "          29       0.43      0.75      0.55         4\n",
      "          30       0.36      0.42      0.38        12\n",
      "          31       0.50      0.54      0.52        13\n",
      "          32       1.00      1.00      1.00        10\n",
      "          33       0.83      1.00      0.91         5\n",
      "          34       0.60      0.43      0.50         7\n",
      "          35       0.33      0.17      0.22         6\n",
      "          36       0.50      0.64      0.56        11\n",
      "          37       0.50      1.00      0.67         2\n",
      "          38       0.33      0.33      0.33         3\n",
      "          39       0.33      0.20      0.25         5\n",
      "          40       0.83      0.50      0.62        10\n",
      "          41       0.62      0.62      0.62         8\n",
      "          42       1.00      0.67      0.80         3\n",
      "          43       0.43      0.50      0.46         6\n",
      "          44       0.80      0.80      0.80         5\n",
      "          45       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.77      2246\n",
      "   macro avg       0.62      0.57      0.57      2246\n",
      "weighted avg       0.77      0.77      0.76      2246\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grbt = GradientBoostingClassifier(random_state=0, verbose=0) # \n",
    "grbt.fit(tfidfv_train, y_train)\n",
    "\n",
    "predicted = grbt.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "print(classification_report(y_test, predicted)) #예측값과 실제값 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dutch-spending",
   "metadata": {},
   "source": [
    "### 4.8 보팅"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "printable-terrain",
   "metadata": {},
   "source": [
    "앙상블 기법이란 여러개의 모델들의 결과들을 혼합하여 더 개선된 결과를 만드는 모델을 만드는 방법을 말합니다.\n",
    "\n",
    "보팅은 앙상블 기법중에 하나로 Soft Voting과 Hard Voting이 존재합니다.\n",
    "- Hard Voting은 일반적인 다수결의 원칙을 따르며 각 모델이 예측한 결과중에서 가장 빈도가 높은 클래스를 결과로 반환하는 방식입니다.\n",
    "[![image](https://user-images.githubusercontent.com/51338268/146578648-c30faa62-2ef8-435e-9492-80e70421651d.png)](https://stats.stackexchange.com/questions/349540/hard-voting-soft-voting-in-ensemble-based-methods)\n",
    "- Soft Voting은 각 모델마다 클래스별 확률값을 가지는 즉, 클래스에 따른 확률분포를 가지는데 모델마다 가지는 클래스별 확률값들을 평균을 내서 가장 높은 클래스를 결과로 반환하는 방식입니다.\n",
    "[![image](https://user-images.githubusercontent.com/51338268/146578469-cb4c34f0-3022-4c94-a1d0-beef43c6856f.png)](https://medium.com/wids-mysore/ensemble-learning-techniques-votingclassifier-c4b38ee62129)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "collaborative-money",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.75      0.75        12\n",
      "           1       0.80      0.77      0.79       105\n",
      "           2       0.67      0.80      0.73        20\n",
      "           3       0.93      0.94      0.93       813\n",
      "           4       0.82      0.88      0.85       474\n",
      "           5       1.00      0.20      0.33         5\n",
      "           6       0.87      0.93      0.90        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.69      0.71      0.70        38\n",
      "           9       0.80      0.80      0.80        25\n",
      "          10       0.90      0.90      0.90        30\n",
      "          11       0.67      0.71      0.69        83\n",
      "          12       0.60      0.46      0.52        13\n",
      "          13       0.69      0.65      0.67        37\n",
      "          14       0.29      1.00      0.44         2\n",
      "          15       0.40      0.22      0.29         9\n",
      "          16       0.73      0.76      0.74        99\n",
      "          17       0.75      0.50      0.60        12\n",
      "          18       0.73      0.55      0.63        20\n",
      "          19       0.71      0.71      0.71       133\n",
      "          20       0.66      0.50      0.57        70\n",
      "          21       0.63      0.81      0.71        27\n",
      "          22       1.00      0.14      0.25         7\n",
      "          23       0.62      0.67      0.64        12\n",
      "          24       0.73      0.58      0.65        19\n",
      "          25       0.92      0.77      0.84        31\n",
      "          26       1.00      1.00      1.00         8\n",
      "          27       0.67      0.50      0.57         4\n",
      "          28       0.33      0.30      0.32        10\n",
      "          29       0.50      1.00      0.67         4\n",
      "          30       0.54      0.58      0.56        12\n",
      "          31       0.82      0.69      0.75        13\n",
      "          32       1.00      1.00      1.00        10\n",
      "          33       0.83      1.00      0.91         5\n",
      "          34       0.80      0.57      0.67         7\n",
      "          35       1.00      0.33      0.50         6\n",
      "          36       0.54      0.64      0.58        11\n",
      "          37       0.50      0.50      0.50         2\n",
      "          38       0.50      0.33      0.40         3\n",
      "          39       0.25      0.20      0.22         5\n",
      "          40       1.00      0.40      0.57        10\n",
      "          41       0.80      0.50      0.62         8\n",
      "          42       1.00      1.00      1.00         3\n",
      "          43       0.83      0.83      0.83         6\n",
      "          44       0.80      0.80      0.80         5\n",
      "          45       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.82      2246\n",
      "   macro avg       0.73      0.66      0.66      2246\n",
      "weighted avg       0.82      0.82      0.81      2246\n",
      "\n"
     ]
    }
   ],
   "source": [
    "voting_classifier = VotingClassifier(estimators=[\n",
    "         ('lr', LogisticRegression(C=10000, penalty='l2')),\n",
    "        ('cb', ComplementNB()),\n",
    "        ('grbt', GradientBoostingClassifier(random_state=0))\n",
    "], voting='soft', n_jobs=-1)\n",
    "voting_classifier.fit(tfidfv_train, y_train)\n",
    "\n",
    "predicted = voting_classifier.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "print(classification_report(y_test, predicted)) #예측값과 실제값 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entire-occupation",
   "metadata": {},
   "source": [
    "## 5. 딥러닝 모델들 성능 비교하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "asian-mother",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 일반 판다스 데이터프레임을 텐서플로우 데이터셋으로 변환\n",
    "def tensorflow_dataset(input_x, input_y, buffer_size, batch_size):\n",
    "    tf_data = tf.data.Dataset.from_tensor_slices((input_x, input_y))\n",
    "    tf_data = tf_data.shuffle(buffer_size)\n",
    "    tf_data = tf_data.repeat()\n",
    "    tf_data = tf_data.batch(batch_size)\n",
    "    tf_data = tf_data.prefetch(buffer_size = -1)\n",
    "    return tf_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "british-pacific",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장의 최대 길이 : 2376\n"
     ]
    }
   ],
   "source": [
    "max_len = np.max(list(map(lambda x: len(x), x_train)))\n",
    "print(f\"문장의 최대 길이 : {max_len}\")\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "embedd_size = 100\n",
    "num_words = np.max(pad_x_train) + 1\n",
    "# 훈련셋\n",
    "pad_x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train, maxlen = max_len)\n",
    "y_tmp = y_train.reshape(-1,1)\n",
    "buffer = len(y_tmp)\n",
    "tf_data = tensorflow_dataset(pad_x_train, y_tmp, buffer, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "presidential-sleeping",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트셋\n",
    "total_num_test = len(x_test)\n",
    "pad_x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test, maxlen = max_len)\n",
    "y_tmp_test = y_test.reshape(-1,1)\n",
    "tf_test = tensorflow_dataset(pad_x_test, y_tmp_test, buffer, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "unlikely-student",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1D(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedd_size):\n",
    "        super(Conv1D, self).__init__()\n",
    "        self.embedd = tf.keras.layers.Embedding(input_dim = vocab_size,\n",
    "                                                output_dim = embedd_size)\n",
    "        self.cnn = tf.keras.layers.Conv1D(64, kernel_size = 3)\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.dense = tf.keras.layers.Dense(64, activation = \"relu\")\n",
    "        self.outputs = tf.keras.layers.Dense(num_classes, activation = \"softmax\")\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.embedd(x)\n",
    "        x = self.cnn(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense(x)\n",
    "        x = self.outputs(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "subtle-honor",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "280/280 [==============================] - 15s 45ms/step - loss: 2.3595 - accuracy: 0.4533\n",
      "Epoch 2/5\n",
      "280/280 [==============================] - 12s 44ms/step - loss: 1.0834 - accuracy: 0.7331\n",
      "Epoch 3/5\n",
      "280/280 [==============================] - 13s 45ms/step - loss: 0.5966 - accuracy: 0.8607\n",
      "Epoch 4/5\n",
      "280/280 [==============================] - 12s 44ms/step - loss: 0.3164 - accuracy: 0.9322\n",
      "Epoch 5/5\n",
      "280/280 [==============================] - 12s 44ms/step - loss: 0.2051 - accuracy: 0.9527\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 1.3466 - accuracy: 0.7205\n",
      "test loss : 1.346612572669983\n",
      "test acc : 0.7205356955528259\n"
     ]
    }
   ],
   "source": [
    "model = Conv1D(num_words, embedd_size)\n",
    "model.compile(loss = \"sparse_categorical_crossentropy\",\n",
    "                  optimizer = \"adam\",\n",
    "                  metrics = [\"accuracy\"])\n",
    "model.fit(tf_data,\n",
    "              steps_per_epoch = buffer // batch_size,\n",
    "              epochs = 5)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(tf_test, steps = total_num_test // batch_size)\n",
    "print(f\"test loss : {test_loss}\")\n",
    "print(f\"test acc : {test_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "certified-edward",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.25      0.33        12\n",
      "           1       0.50      0.78      0.61       105\n",
      "           2       0.47      0.40      0.43        20\n",
      "           3       0.88      0.92      0.90       813\n",
      "           4       0.80      0.84      0.82       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.80      0.57      0.67        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.56      0.71      0.63        38\n",
      "           9       0.83      0.60      0.70        25\n",
      "          10       0.58      0.70      0.64        30\n",
      "          11       0.66      0.58      0.62        83\n",
      "          12       0.40      0.15      0.22        13\n",
      "          13       0.53      0.43      0.48        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.47      0.75      0.58        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.50      0.30      0.37        20\n",
      "          19       0.60      0.68      0.64       133\n",
      "          20       0.58      0.40      0.47        70\n",
      "          21       0.47      0.33      0.39        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.00      0.00      0.00        12\n",
      "          24       0.40      0.11      0.17        19\n",
      "          25       0.73      0.35      0.48        31\n",
      "          26       0.00      0.00      0.00         8\n",
      "          27       1.00      0.25      0.40         4\n",
      "          28       0.06      0.10      0.07        10\n",
      "          29       0.33      0.25      0.29         4\n",
      "          30       0.33      0.08      0.13        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       0.67      0.20      0.31        10\n",
      "          33       0.80      0.80      0.80         5\n",
      "          34       0.67      0.29      0.40         7\n",
      "          35       0.50      0.17      0.25         6\n",
      "          36       0.00      0.00      0.00        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.33      0.33      0.33         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.67      0.20      0.31        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.18      0.67      0.29         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.72      2246\n",
      "   macro avg       0.42      0.33      0.34      2246\n",
      "weighted avg       0.71      0.72      0.70      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(pad_x_test)\n",
    "y_pred = np.argmax(y_pred, axis = 1)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
